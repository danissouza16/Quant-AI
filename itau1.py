# -*- coding: utf-8 -*-
"""itau1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UH_mTW33ZWLGuzEmEujiDmDfI2ismJqK
"""

# Célula 1 (Atualizada): Instalação das Bibliotecas
!pip install yfinance pytrends arch -q

# Célula 2: Importações
import pandas as pd
import numpy as np
import yfinance as yf
from pytrends.request import TrendReq
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Configurações de visualização
sns.set(style='whitegrid')
plt.rcParams['figure.figsize'] = (15, 8)

# Célula 3: Parâmetros e Configuração da Estratégia

# --- Parâmetros do Ativo e Período ---
TICKER = 'NVDA'
START_DATE = '2018-01-01'
END_DATE = datetime.today().strftime('%Y-%m-%d')

# --- Parâmetros do Fator de Hype (Google Trends) ---
GTRENDS_KEYWORDS = ['Nvidia stock', 'NVDA', 'Nvidia AI']

# --- Parâmetros da Estratégia ---
# Horizonte de previsão: quantos dias à frente queremos prever.
# 5 dias = 1 semana de negociação.
PREDICTION_HORIZON_DAYS = 5

# Limiares para classificação: retorno > 1% = Sobe, retorno < -1% = Desce
UP_THRESHOLD = 0.01
DOWN_THRESHOLD = -0.01

# Célula 3.1: Parâmetros de Backtest e Custos

# --- Parâmetros de Capital e Posição ---
INITIAL_CAPITAL = 100000.00

# --- Parâmetros de Custos Transacionais ---
BROKERAGE_FEE = 4.50
B3_FEES_PCT = 0.0003  # 0.03%
SLIPPAGE_PCT = 0.0005 # 0.05% ou 5 basis points

# --- Parâmetros Fiscais (Tributação) ---
# Alíquota de Imposto de Renda sobre o ganho de capital líquido para swing trade
INCOME_TAX_RATE = 0.15 # 15%

# Célula 4 (Corrigida e Robusta): Coleta de Dados do Ativo (yfinance)
print(f"Baixando dados para o ticker {TICKER} de {START_DATE} a {END_DATE}...")

# Baixa os dados OHLCV
stock_data = yf.download(TICKER, start=START_DATE, end=END_DATE)

# --- CORREÇÃO PARA O MergeError ---
# Verifica se as colunas são um MultiIndex (estrutura de múltiplos níveis)
if isinstance(stock_data.columns, pd.MultiIndex):
    print("MultiIndex detectado nas colunas. Simplificando para um único nível...")
    # Mantém apenas o primeiro nível do índice de colunas (ex: 'Open', 'Close')
    stock_data.columns = stock_data.columns.get_level_values(0)
    print("Colunas simplificadas.")

if stock_data.empty:
    print("Nenhum dado foi baixado. Verifique o ticker ou o período.")
else:
    print("\nDados baixados com sucesso!")
    print(stock_data.tail())

# Célula 5 (VERSÃO FINAL E CORRIGIDA - Anti-Duplicatas)

import pandas as pd
import os
from datetime import datetime

MASTER_CSV_PATH = 'master_google_trends_data.csv'

print("--- Processador de Dados do Google Trends ---")

if os.path.exists(MASTER_CSV_PATH):
    print(f"Arquivo mestre encontrado! Carregando dados de '{MASTER_CSV_PATH}'...")
    trends_data = pd.read_csv(MASTER_CSV_PATH, index_col='date', parse_dates=True)
    print("Dados carregados com sucesso do arquivo mestre.")

else:
    print("Arquivo mestre não encontrado. Tentando montar a partir de arquivos CSV individuais...")

    keywords = ['Nvidia stock', 'NVDA', 'Nvidia AI']
    years = range(2018, datetime.today().year + 1)

    all_keyword_dfs = {}

    try:
        for keyword in keywords:
            yearly_dfs = []
            keyword_filename_base = keyword.replace(' ', '_')

            for year in years:
                filename = f"{keyword_filename_base}_{year}.csv"
                if os.path.exists(filename):
                    df_year = pd.read_csv(filename, skiprows=1, index_col=0, parse_dates=True)
                    df_year.columns = [keyword]
                    yearly_dfs.append(df_year)
                else:
                    print(f"AVISO: Arquivo '{filename}' não foi encontrado. Pulando.")

            if yearly_dfs:
                # Concatena os dataframes anuais
                keyword_df_with_duplicates = pd.concat(yearly_dfs)

                # --- A CORREÇÃO ESTÁ AQUI ---
                # Remove quaisquer datas duplicadas que possam ter surgido da sobreposição de arquivos
                keyword_df = keyword_df_with_duplicates[~keyword_df_with_duplicates.index.duplicated(keep='first')]

                all_keyword_dfs[keyword] = keyword_df

        if not all_keyword_dfs:
            raise FileNotFoundError("Nenhum arquivo CSV individual foi encontrado para processar.")

        trends_data = pd.concat(all_keyword_dfs.values(), axis=1)

        trends_data = trends_data.asfreq('D', method='ffill')
        trends_data.index.name = 'date'

        trends_data['hype_factor'] = trends_data[keywords].sum(axis=1)

        trends_data.to_csv(MASTER_CSV_PATH)
        print(f"\nSUCESSO! Arquivo mestre '{MASTER_CSV_PATH}' foi criado e salvo.")

    except Exception as e:
        print(f"\nFALHA: Ocorreu um erro durante a montagem do dataset: {e}")
        print("Verifique se os arquivos CSV foram upados e nomeados corretamente.")
        trends_data = pd.DataFrame()

if not trends_data.empty:
    print("\n--- Amostra do DataFrame Mestre Montado ---")
    print(trends_data.tail())

# Célula 6 (Versão Final com GARCH): Engenharia de Features Avançada

# Importa a biblioteca arch
from arch import arch_model

# --- 1. Preparação do DataFrame Principal ---
df = stock_data[['Close', 'Volume']].copy()
df['daily_return'] = df['Close'].pct_change()

# --- 2. Criação do Fator de Volatilidade GARCH ---
# Prepara a série de retornos para o modelo GARCH (remove o primeiro NaN e multiplica por 100 para estabilidade numérica)
returns_for_garch = df['daily_return'].dropna() * 100

# Define o modelo GARCH(1,1). p=1 e q=1 são os parâmetros mais comuns.
# 't' para a distribuição de Student, que é mais realista para retornos financeiros (caudas gordas).
garch_model = arch_model(returns_for_garch, p=1, q=1, vol='Garch', dist='t')

# Treina o modelo nos dados. 'disp="off"' evita imprimir o log de convergência.
garch_results = garch_model.fit(disp="off")

# Extrai a volatilidade condicional (em escala diária, como %)
# E a anualiza (multiplicando pela raiz do número de dias de negociação no ano, ~252)
# Reindexamos para garantir o alinhamento com o DataFrame principal 'df'
df['volatility_garch'] = (garch_results.conditional_volatility / 100) * np.sqrt(252)

print("Modelo GARCH treinado e feature de volatilidade criada.")


# --- 3. Criação dos Fatores Quantitativos Clássicos ---
momentum_windows = [30, 60, 90]
for window in momentum_windows:
    df[f'momentum_{window}d'] = df['daily_return'].rolling(window=window).sum()


# --- 4. Integração do Fator de Hype ---
if not trends_data.empty:
    df = pd.merge(df, trends_data[['hype_factor']], left_index=True, right_index=True, how='left')
    df['hype_factor'] = df['hype_factor'].ffill()
    df['hype_change_1w'] = df['hype_factor'].pct_change(periods=7)

# --- 5. Criação da Variável Alvo (Target) ---
df['future_return'] = df['Close'].pct_change(periods=PREDICTION_HORIZON_DAYS).shift(-PREDICTION_HORIZON_DAYS)

# Converte o retorno futuro em classes
conditions = [
    df['future_return'] > UP_THRESHOLD,
    df['future_return'] < DOWN_THRESHOLD
]
choices = ['Sobe', 'Desce']
df['target_class'] = np.select(conditions, choices, default='Neutro')

print("\nEngenharia de features finalizada. Veja as colunas criadas:")
print(df.tail(10))

# Célula 7 (Atualizada para GARCH): Preparação Final do Dataset

# --- CORREÇÃO: A lista de features agora busca por 'volatility_garch' ---
features = [col for col in df.columns if 'momentum' in col or 'volatility_garch' in col or 'hype' in col]
target = 'target_class'

# Remove colunas que não são features ou alvo para limpar o dataset
cols_to_drop = ['Close', 'Volume', 'daily_return', 'future_return']
model_df = df.drop(columns=cols_to_drop)

# Remove todas as linhas que contêm valores nulos (NaN)
original_rows = len(model_df)
model_df.dropna(inplace=True)
print(f"Dataset limpo. {original_rows - len(model_df)} linhas com dados nulos foram removidas.")

# Separa os dados em X (features) e y (alvo)
X = model_df[features]
y = model_df[target]

print("\nShape de X (features):", X.shape)
print("Shape de y (alvo):", y.shape)
print("\nFeatures que serão usadas pelo modelo:")
print(X.columns.tolist())
print("\nDistribuição das classes do alvo:")
print(y.value_counts(normalize=True))

# Célula 8 (Corrigida para GARCH): Visualização das Features Criadas

fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15, 20), sharex=True)
fig.suptitle('Visualização das Features e do Preço do Ativo', fontsize=16)

# Gráfico 1: Preço de Fechamento
axes[0].plot(df.loc[X.index, 'Close'], label='Preço de Fechamento')
axes[0].set_title(f'Preço da {TICKER}')
axes[0].legend()

# Gráfico 2: Fatores de Momentum
for col in X.columns:
    if 'momentum' in col:
        axes[1].plot(X[col], label=col)
axes[1].set_title('Fatores de Momentum')
axes[1].legend()

# --- CORREÇÃO ---
# Gráfico 3: Fator de Volatilidade GARCH
# Plotamos diretamente a coluna 'volatility_garch' que agora existe em X
axes[2].plot(X['volatility_garch'], label='Volatilidade Anualizada GARCH(1,1)', color='purple')
axes[2].set_title('Fator de Volatilidade (GARCH)')
axes[2].legend()

# Gráfico 4: Fator de Hype
if 'hype_factor' in X.columns:
    ax4_twin = axes[3].twinx()
    axes[3].plot(X['hype_factor'], label='Interesse de Busca (Google Trends)', color='orange')
    ax4_twin.plot(X['hype_change_1w'], label='Variação Semanal do Hype', color='red', linestyle='--')
    axes[3].set_title('Fator de Hype')
    axes[3].legend(loc='upper left')
    ax4_twin.legend(loc='upper right')

plt.tight_layout(rect=[0, 0.03, 1, 0.98])
plt.show()

# Célula 9: Análise de Correlação

# Vamos analisar a correlação das features com o retorno numérico, antes da classificação
# Precisamos garantir que os índices estão alinhados
analysis_df = X.copy()
analysis_df['future_return'] = df.loc[X.index, 'future_return']

# Calcula a matriz de correlação
correlation_matrix = analysis_df.corr()

# Foca na correlação das features com o retorno futuro
hype_correlation = correlation_matrix[['future_return']].sort_values(by='future_return', ascending=False)

print("Correlação das Features com o Retorno Futuro:")
print(hype_correlation)

# Visualiza a matriz de correlação completa
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlação entre Features e Retorno Futuro')
plt.show()

# Célula 10 (Reformulada): Análise Visual das Features de Hype

# Junta as features com o alvo para facilitar a plotagem
plot_df = X.copy()
plot_df['target_class'] = y

# Configura a figura para ter dois gráficos lado a lado
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))
fig.suptitle('Distribuição das Features de Hype por Classe Alvo', fontsize=18)

# --- Gráfico 1: Variação do Hype ---
sns.boxplot(ax=axes[0], data=plot_df, x='target_class', y='hype_change_1w', order=['Desce', 'Neutro', 'Sobe'])
axes[0].set_title('Análise da Feature "hype_change_1w" (Variação)', fontsize=14)
axes[0].set_xlabel('Classe do Retorno Futuro')
axes[0].set_ylabel('Distribuição da Variação Semanal do Hype')

# --- Gráfico 2: Nível do Hype ---
sns.boxplot(ax=axes[1], data=plot_df, x='target_class', y='hype_factor', order=['Desce', 'Neutro', 'Sobe'])
axes[1].set_title('Análise da Feature "hype_factor" (Nível)', fontsize=14)
axes[1].set_xlabel('Classe do Retorno Futuro')
axes[1].set_ylabel('Distribuição do Nível do Hype')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Célula 11 (Reformulada): Teste Estatístico (ANOVA) para Features de Hype
from scipy.stats import f_oneway

print("--- Teste ANOVA para as Features de Hype ---")

# Lista das features de hype que queremos testar
features_to_test = ['hype_change_1w', 'hype_factor']

# Cria o DataFrame para análise se ele não existir
if 'plot_df' not in locals() or not isinstance(plot_df, pd.DataFrame):
    plot_df = X.copy()
    plot_df['target_class'] = y

# Loop para executar o teste para cada feature
for feature in features_to_test:
    # Separa os dados da feature para cada classe
    group_sobe = plot_df[plot_df['target_class'] == 'Sobe'][feature]
    group_desce = plot_df[plot_df['target_class'] == 'Desce'][feature]
    group_neutro = plot_df[plot_df['target_class'] == 'Neutro'][feature]

    # Realiza o teste ANOVA
    f_statistic, p_value = f_oneway(group_sobe, group_desce, group_neutro)

    print(f"\n--- Feature: {feature} ---")
    print(f"Estatística F: {f_statistic:.4f}")
    print(f"P-valor: {p_value:.4f}")

    if p_value < 0.05:
        print("Conclusão: P-valor < 0.05. Rejeitamos a hipótese nula.")
        print("=> Há uma diferença estatisticamente significativa nas médias da feature entre as classes.")
        print("=> Sinal preditivo confirmado.")
    else:
        print("Conclusão: P-valor >= 0.05. Não podemos rejeitar a hipótese nula.")
        print("=> Não há evidência de diferença significativa nas médias.")

# Célula 12: Análise Visual (Box Plot) dos Fatores Quantitativos

# Features quantitativas para analisar
quant_features = ['volatility_garch', 'momentum_30d', 'momentum_60d', 'momentum_90d']

# Cria o DataFrame para plotagem se ele não existir
if 'plot_df' not in locals():
    plot_df = X.copy()
    plot_df['target_class'] = y

# Configura os gráficos (2x2)
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 14))
fig.suptitle('Distribuição dos Fatores Quantitativos por Classe Alvo', fontsize=18)

# "Achatando" o array de eixos para facilitar o loop
axes = axes.flatten()

for i, feature in enumerate(quant_features):
    sns.boxplot(ax=axes[i], data=plot_df, x='target_class', y=feature, order=['Desce', 'Neutro', 'Sobe'])
    axes[i].set_title(f'Análise da Feature "{feature}"', fontsize=14)
    axes[i].set_xlabel('Classe do Retorno Futuro')
    axes[i].set_ylabel('Distribuição da Feature')

plt.tight_layout(rect=[0, 0.03, 1, 0.96])
plt.show()

# Célula 13: Teste Estatístico (ANOVA) dos Fatores Quantitativos
from scipy.stats import f_oneway

print("--- Teste ANOVA para as Features Quantitativas ---")

# Features para testar
features_to_test = ['volatility_garch', 'momentum_30d', 'momentum_60d', 'momentum_90d']

for feature in features_to_test:
    # Separa os dados da feature para cada classe
    group_sobe = plot_df[plot_df['target_class'] == 'Sobe'][feature]
    group_desce = plot_df[plot_df['target_class'] == 'Desce'][feature]
    group_neutro = plot_df[plot_df['target_class'] == 'Neutro'][feature]

    # Realiza o teste ANOVA
    f_statistic, p_value = f_oneway(group_sobe, group_desce, group_neutro)

    print(f"\n--- Feature: {feature} ---")
    print(f"Estatística F: {f_statistic:.4f}")
    print(f"P-valor: {p_value:.4f}")

    if p_value < 0.05:
        print("Conclusão: P-valor < 0.05. Há uma diferença estatisticamente significativa nas médias.")
        print("=> Sinal preditivo confirmado.")
    else:
        print("Conclusão: P-valor >= 0.05. Não há evidência de diferença significativa nas médias.")
        print("=> Sinal preditivo isolado não é estatisticamente significativo.")

# Célula 14: Análise de Estacionariedade e Distribuição das Features

from statsmodels.tsa.stattools import adfuller

print("--- 1. Teste de Estacionariedade (Augmented Dickey-Fuller) ---")
print("Hipótese Nula (H0): A série NÃO é estacionária (possui raiz unitária).")
print("Um P-valor < 0.05 indica que podemos rejeitar H0 e considerar a série estacionária.\n")

# Usamos o DataFrame 'X' que foi refinado na célula 8.1
for feature in X.columns:
    result = adfuller(X[feature])
    p_value = result[1]

    print(f"Feature: {feature}")
    print(f"  - ADF Statistic: {result[0]:.4f}")
    print(f"  - P-valor: {p_value:.4f}")

    if p_value < 0.05:
        print("  - Conclusão: A série é ESTACIONÁRIA.")
    else:
        print("  - Conclusão: A série NÃO é estacionária.")
    print("-" * 30)

print("\n--- 2. Visualização da Distribuição das Features ---")

num_features = len(X.columns)
fig, axes = plt.subplots(nrows=(num_features + 1) // 2, ncols=2, figsize=(18, 5 * ((num_features + 1) // 2)))
axes = axes.flatten()

for i, feature in enumerate(X.columns):
    sns.histplot(X[feature], ax=axes[i], kde=True, bins=50)
    axes[i].set_title(f'Distribuição da Feature: {feature}', fontsize=14)
    axes[i].set_xlabel('Valor')
    axes[i].set_ylabel('Frequência')

for i in range(num_features, len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

# Célula 15 (Atualizada): Análise do Decaimento do Sinal (Information Decay)

print("--- Análise do Decaimento do Sinal (Information Coefficient ao Longo do Tempo) ---")

# --- ALTERAÇÃO AQUI: Adicionamos 'hype_factor' à lista de análise ---
features_for_decay_analysis = ['hype_change_1w', 'hype_factor', 'momentum_30d', 'volatility_garch']

horizons = range(1, 16)
ic_results = pd.DataFrame(index=horizons, columns=features_for_decay_analysis)

# Usa o DataFrame 'df' original, alinhado com o índice de 'X'
analysis_df = df.loc[X.index].copy()

for n in horizons:
    future_return_n = analysis_df['daily_return'].rolling(window=n).sum().shift(-n)

    for feature in features_for_decay_analysis:
        ic = analysis_df[feature].corr(future_return_n)
        ic_results.loc[n, feature] = ic

ic_results = ic_results.astype(float)

print("\nCoeficiente de Informação (Correlação) para cada horizonte futuro:")
print(ic_results)

ax = ic_results.plot(kind='bar', figsize=(18, 8), colormap='viridis')
ax.axhline(0, color='black', linewidth=0.8, linestyle='--')
ax.set_title('Decaimento do Sinal Preditivo das Features (Information Decay)', fontsize=16)
ax.set_xlabel('Horizonte de Previsão Futuro (dias)')
ax.set_ylabel('Information Coefficient (Correlação)')
ax.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Célula 16 (Nova e Definitiva): Loop de Experimentação de Classificadores

import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import classification_report, f1_score

# --- 1. Definição dos Conjuntos de Features para Testar ---
feature_sets = {
    "All_Features": [
        'volatility_garch', 'momentum_30d', 'momentum_60d', 'momentum_90d', 'hype_factor', 'hype_change_1w'
    ],
    "Classic_Quant": [
        'volatility_garch', 'momentum_30d', 'momentum_60d', 'momentum_90d'
    ],
    "Hype_Only": [
        'hype_factor', 'hype_change_1w'
    ],
    "Minimalist_Contrarian": [
        'hype_factor'
    ]
}

# Dicionário para armazenar os relatórios de classificação
classification_summaries = {}

# --- 2. Preparação dos Dados (feito uma vez) ---
y = df.loc[X.index, 'target_class']
split_index = int(len(X) * 0.8)
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

encoder = LabelEncoder()
encoder.fit(y)
y_train_encoded = encoder.transform(y_train)
sample_weights = compute_sample_weight('balanced', y=y_train_encoded)

# --- 3. O Loop de Experimentação ---
for model_name, feature_list in feature_sets.items():
    print(f"\n{'='*25} AVALIANDO MODELO: {model_name} {'='*25}")

    # --- a. Seleção das Features ---
    X_model = X[feature_list]
    X_train_model, X_test_model = X_model.iloc[:split_index], X_model.iloc[split_index:]

    print(f"Features utilizadas: {feature_list}")

    # --- b. Treinamento do Classificador ---
    xgb_classifier = xgb.XGBClassifier(
        objective='multi:softmax', num_class=3, eval_metric='mlogloss',
        max_depth=3, subsample=0.8, colsample_bytree=0.8,
        use_label_encoder=False, random_state=42
    )
    xgb_classifier.fit(X_train_model, y_train_encoded, sample_weight=sample_weights)

    # --- c. Geração e Avaliação das Previsões ---
    predictions = xgb_classifier.predict(X_test_model)
    predictions_labels = encoder.inverse_transform(predictions)

    report = classification_report(y_test, predictions_labels, output_dict=True)
    classification_summaries[model_name] = {
        'accuracy': report['accuracy'],
        'precision_neutro': report['Neutro']['precision'],
        'recall_neutro': report['Neutro']['recall'],
        'f1_score_neutro': report['Neutro']['f1-score'],
        'recall_desce': report['Desce']['recall']
    }

    print(f"Acurácia: {report['accuracy']:.2f}")
    print(f"Recall da Classe 'Neutro': {report['Neutro']['recall']:.2f}")
    print(f"Recall da Classe 'Desce': {report['Desce']['recall']:.2f}")

# --- 4. Apresentação Final dos Resultados Comparativos ---
print(f"\n{'='*25} COMPARAÇÃO FINAL DOS CLASSIFICADORES {'='*25}")

summary_df = pd.DataFrame.from_dict(classification_summaries, orient='index')
summary_df.sort_values(by='f1_score_neutro', ascending=False, inplace=True)

# Formatando para melhor visualização
summary_df_display = summary_df.style.format({
    'accuracy': '{:.2%}',
    'precision_neutro': '{:.2%}',
    'recall_neutro': '{:.2%}',
    'f1_score_neutro': '{:.2f}',
    'recall_desce': '{:.2%}'
})

print("O melhor modelo é aquele que demonstra um melhor equilíbrio, especialmente no 'recall' das classes 'Neutro' e 'Desce'.")
display(summary_df_display)