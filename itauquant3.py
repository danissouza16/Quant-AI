# -*- coding: utf-8 -*-
"""itauquant3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-LIgBEhPFAkOBlGnMA8rgCAi3VtdPc5x
"""

# --- Importação das Bibliotecas ---
import pandas as pd
import numpy as np
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# --- Configuração de Visualização ---
# Para não usar notação científica e facilitar a leitura
pd.options.display.float_format = '{:,.2f}'.format
# Define um estilo visual para os gráficos
plt.style.use('seaborn-v0_8-whitegrid')

print("--- Carregando o Dataset Final ---")
# Certifique-se de que o nome do arquivo corresponde ao que você salvou
try:
    df = pd.read_csv('nvda.csv', index_col='data', parse_dates=True)
    print("✅ Dataset carregado com sucesso!")
    print(f"O dataset tem {df.shape[0]} linhas e {df.shape[1]} colunas.")
    print("Visualização inicial:")
    print(df.head())
except FileNotFoundError:
    print("❌ ERRO: Arquivo 'nvda.csv' não encontrado. Verifique o nome ou faça o upload.")

# --- Criando a Variável Alvo (Y) ---

# .shift(-1) "puxa" o preço de fecho de amanhã para a linha de hoje
# Comparamos o preço de amanhã com o de hoje para ver se subiu (True) ou não (False)
df['target'] = (df['Close'].shift(-1) > df['Close'])

# Convertemos True/False para 1/0
df['target'] = df['target'].astype(int)

print("Coluna 'target' criada com sucesso.")
print("Visualização do 'Close' vs 'target' (1 = dia seguinte subiu, 0 = caiu/manteve):")
# Note como o target da primeira linha corresponde à variação do Close entre a primeira e a segunda linha
print(df[['Close', 'target']].head(10))

# A última linha terá um target NaN (pois não há dia seguinte), então a removemos
df.dropna(subset=['target'], inplace=True)
print(f"\nÚltima linha removida. Novo total de linhas: {len(df)}")

# --- Análise de Correlação ---
print("\nGerando mapa de calor de correlação entre as features...")

# Calcula a matriz de correlação
corr_matrix = df.corr()

# Cria a figura para o gráfico
plt.figure(figsize=(18, 10))

# Gera o mapa de calor
sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)
plt.title('Mapa de Calor de Correlação de Todas as Features', fontsize=18)
plt.show()

# --- Análise de Importância das Features (Feature Importance) ---

print("\nIniciando análise de Feature Importance...")

# 1. Separar o dataset em Features (X) e Alvo (Y)
X = df.drop(columns=['target'])
Y = df['target']

# 2. Dividir os dados em treino e teste (SEM embaralhar, para manter a ordem temporal)
# Vamos usar os primeiros 80% dos dados para treinar o modelo de análise
split_index = int(len(X) * 0.8)
X_train, X_test = X[:split_index], X[split_index:]
Y_train, Y_test = Y[:split_index], Y[split_index:]

print(f"Dados divididos em {len(X_train)} linhas de treino e {len(X_test)} linhas de teste.")

# 3. Treinar um modelo RandomForest para obter a importância das features
# RandomForest é ótimo para esta tarefa
model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
print("Treinando modelo RandomForest preliminar...")
model_rf.fit(X_train, Y_train)
print("Treinamento concluído.")

# 4. Extrair e visualizar a importância
importances = model_rf.feature_importances_
feature_names = X.columns

# Criar um DataFrame para facilitar a visualização
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# 5. Gerar o gráfico
plt.figure(figsize=(12, 10))
sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')
plt.title('Feature Importance para o Modelo', fontsize=18)
plt.xlabel('Importância', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.tight_layout()
plt.show()

# --- Selecionando as Features Mais Importantes (com base na sua análise) ---
# Esta lista é uma representação das features que se destacaram.
top_features = [
    'garch_volatility',
    'Volume',
    'RSI',
    'CUDA_zscore_90d',
    'Nvidia_zscore_90d',
    'Indice_Medo_VIX',
    'Spread_Curva_Juros',
    'Preco_Petroleo_WTI',
    'Indice_Dolar_USD',
    'Geforce_sma_7d',
    'Close', # Mantemos o 'Close' para calcular o alvo e os retornos
    'Indice_Semicondutores',
    'sentimento_reddit',
    'Juros_10A_EUA',
    'SMA_20'
]

df_model = df[top_features].copy()
print(f"Dataset reduzido para as {len(top_features)} features mais importantes.")

# --- Criando a Variável Alvo (Y) ---
# Prever se o preço de fecho do dia seguinte (`shift(-1)`) será maior que o de hoje
df_model['target'] = (df_model['Close'].shift(-1) > df_model['Close']).astype(int)

# A última linha terá um target NaN (pois não há dia seguinte), então a removemos
df_model.dropna(inplace=True)
print("✅ Variável alvo 'target' criada e NaNs removidos.")

# --- Separando Features (X) e Alvo (Y) ---
Y = df_model['target']
# Removemos o 'target' e o 'Close' das features para evitar vazamento de dados
X = df_model.drop(columns=['target', 'Close'])

# --- Divisão Temporal (Treino e Teste) ---
split_percentage = 0.8
split_index = int(len(X) * split_percentage)

X_train, X_test = X[:split_index], X[split_index:]
Y_train, Y_test = Y[:split_index], Y[split_index:]

print(f"Dados divididos em {len(X_train)} amostras de treino e {len(X_test)} amostras de teste.")
print(f"Período de treino: {X_train.index.min().date()} a {X_train.index.max().date()}")
print(f"Período de teste:  {X_test.index.min().date()} a {X_test.index.max().date()}")

# --- Treinamento do Modelo XGBoost ---
print("\nTreinando modelo XGBoost...")
model_xgb = xgb.XGBClassifier(objective='binary:logistic',
                              eval_metric='logloss',
                              use_label_encoder=False,
                              random_state=42)

model_xgb.fit(X_train, Y_train)
print("✅ Modelo treinado com sucesso.")

# --- Geração de Sinais de Trading ---
# Usamos .predict_proba() para obter a confiança do modelo
# [:, 1] pega a probabilidade da classe '1' (alta)
probabilities = model_xgb.predict_proba(X_test)[:, 1]

# Definimos os limiares para a nossa estratégia
buy_threshold = 0.55
sell_threshold = 0.45

# Criamos os sinais
signals = pd.Series(0, index=X_test.index, name='signals') # Começa com "Manter" (0)
signals[probabilities > buy_threshold] = 1  # Sinal de Compra
signals[probabilities < sell_threshold] = -1 # Sinal de Venda

print("✅ Sinais de trading gerados para o período de teste.")

# --- Simulação do Backtest Vetorizado ---
# Pegamos os preços de fecho do período de teste
close_prices = df_model.loc[X_test.index]['Close']

# Calculamos os retornos diários do ativo
asset_returns = close_prices.pct_change()

# O retorno da nossa estratégia é o retorno do ativo multiplicado pelo sinal do dia ANTERIOR
# .shift(1) é crucial para evitar olhar para o futuro (usamos o sinal de hoje para a operação de amanhã)
strategy_returns = signals.shift(1) * asset_returns

# --- Análise de Performance ---
print("\n--- MÉTRICAS DE PERFORMANCE DA ESTRATÉGIA ---")

# 1. Retorno Total
total_return = (1 + strategy_returns).cumprod().iloc[-1] - 1
print(f"Retorno Total da Estratégia: {total_return:.2%}")

# 2. Sharpe Ratio (Anualizado)
# (Assumindo taxa livre de risco de 0)
sharpe_ratio = (strategy_returns.mean() / strategy_returns.std()) * np.sqrt(252)
print(f"Índice de Sharpe (anualizado): {sharpe_ratio:.2f}")

# 3. Máximo Drawdown
equity_curve = (1 + strategy_returns).cumprod()
rolling_max = equity_curve.cummax()
drawdown = (equity_curve - rolling_max) / rolling_max
max_drawdown = drawdown.min()
print(f"Máximo Drawdown: {max_drawdown:.2%}")


# --- Visualização dos Resultados ---
plt.figure(figsize=(15, 8))
# Curva de capital da estratégia
((1 + strategy_returns).cumprod()).plot(label='Minha Estratégia', lw=2)
# Curva de capital do Buy & Hold (comprar e segurar)
((1 + asset_returns).cumprod()).plot(label='Buy & Hold (Comprar e Segurar)', lw=2, linestyle='--')
plt.title('Performance da Estratégia vs. Buy & Hold', fontsize=18)
plt.ylabel('Retorno Cumulativo')
plt.legend()
plt.show()

# Célula de Treinamento e Backtest (VERSÃO MELHORADA)

# --- Treinamento do Modelo XGBoost (com janela de treino mais recente) ---
print("\nTreinando modelo XGBoost com dados mais recentes...")

# --- MUDANÇA 1: JANELA DE TREINO ADAPTATIVA ---
# Em vez de X_train e Y_train inteiros, pegamos apenas os últimos 3 anos de dados de treino
anos_de_treino = 3
linhas_de_treino = anos_de_treino * 252 # 252 dias de negociação por ano

X_train_recente = X_train.tail(linhas_de_treino)
Y_train_recente = Y_train.tail(linhas_de_treino)

print(f"Usando um período de treino mais recente: {X_train_recente.index.min().date()} a {X_train_recente.index.max().date()}")

model_xgb = xgb.XGBClassifier(objective='binary:logistic',
                              eval_metric='logloss',
                              use_label_encoder=False,
                              random_state=42)

model_xgb.fit(X_train_recente, Y_train_recente)
print("✅ Modelo treinado com sucesso.")

# --- Geração de Sinais de Trading (com estratégia Long-Only) ---
probabilities = model_xgb.predict_proba(X_test)[:, 1]

# --- MUDANÇA 2: ESTRATÉGIA LONG-ONLY ---
buy_threshold = 0.55 # Mantemos o limiar de compra

# Começa com "Manter" (0)
signals = pd.Series(0, index=X_test.index, name='signals')
# Só agimos no sinal de COMPRA. Nunca vendemos (short).
signals[probabilities > buy_threshold] = 1

print(f"✅ Sinais de trading 'Long-Only' gerados. Total de {len(signals[signals==1])} sinais de compra.")

# --- Simulação do Backtest e Análise de Performance (código igual ao anterior) ---
close_prices = df_model.loc[X_test.index]['Close']
asset_returns = close_prices.pct_change()
strategy_returns = signals.shift(1) * asset_returns

print("\n--- MÉTRICAS DE PERFORMANCE DA ESTRATÉGIA MELHORADA ---")
total_return = (1 + strategy_returns).cumprod().iloc[-1] - 1
print(f"Retorno Total da Estratégia: {total_return:.2%}")
sharpe_ratio = (strategy_returns.mean() / strategy_returns.std()) * np.sqrt(252)
print(f"Índice de Sharpe (anualizado): {sharpe_ratio:.2f}")
equity_curve = (1 + strategy_returns).cumprod()
rolling_max = equity_curve.cummax()
drawdown = (equity_curve - rolling_max) / rolling_max
max_drawdown = drawdown.min()
print(f"Máximo Drawdown: {max_drawdown:.2%}")

# --- Visualização dos Resultados ---
plt.figure(figsize=(15, 8))
((1 + strategy_returns).cumprod()).plot(label='Minha Estratégia Melhorada', lw=2)
((1 + asset_returns).cumprod()).plot(label='Buy & Hold', lw=2, linestyle='--')
plt.title('Performance da Estratégia Melhorada vs. Buy & Hold', fontsize=18)
plt.ylabel('Retorno Cumulativo')
plt.legend()
plt.show()

import numpy as np
# --- PARÂMETROS DE AJUSTE DA ESTRATÉGIA (Sinta-se à vontade para experimentar aqui) ---

# 1. Orçamento de Risco: Aumentado de 0.02 para 0.03 (permite posições maiores em geral)
VOLATILITY_TARGET = 0.03

# 2. Amplificador de Confiança: Multiplica o sinal da probabilidade para torná-lo mais agressivo.
CONVICTION_SCALER = 3.0

# 3. Limite de Segurança: Evita que a alavancagem fique excessivamente alta.
MAX_LEVERAGE = 1.5

# 4. Limiar de Compra (mantido)
BUY_THRESHOLD = 0.55

# --- 1. Geração de Probabilidades (igual ao anterior) ---
print("Gerando probabilidades com o modelo XGBoost treinado...")
probabilities = model_xgb.predict_proba(X_test)[:, 1]

# --- 2. Cálculo da Posição Híbrida REFINADA ---
print("Calculando o tamanho da posição híbrida REFINADA...")
garch_volatility_test = X_test['garch_volatility']

# Camada 1: O Gerente de Risco (GARCH) define o tamanho MÁXIMO da posição
base_position_size = VOLATILITY_TARGET / (garch_volatility_test / 100)

# Camada 2: O Otimizador de Oportunidade (XGBoost) define o "acelerador"
conviction_multiplier = (probabilities - BUY_THRESHOLD) / (1.0 - BUY_THRESHOLD)
conviction_multiplier = np.clip(conviction_multiplier, 0, 1)

# APLICANDO O AMPLIFICADOR DE CONFIANÇA
scaled_conviction = conviction_multiplier * CONVICTION_SCALER

# A posição final é o orçamento de risco multiplicado pelo nosso nível de convicção amplificado
final_position = base_position_size * scaled_conviction
# Aplicamos o nosso limite de segurança
final_position = np.clip(final_position, 0, MAX_LEVERAGE)
print("✅ Posição híbrida refinada calculada para cada dia.")

# --- 3. Simulação do Backtest Vetorizado ---
close_prices = df_model.loc[X_test.index]['Close']
asset_returns = close_prices.pct_change()
strategy_returns = final_position.shift(1) * asset_returns
strategy_returns.fillna(0, inplace=True) # Preenche o primeiro NaN com 0

# --- 4. Análise de Performance ---
print("\n--- MÉTRICAS DE PERFORMANCE DA ESTRATÉGIA HÍBRIDA REFINADA ---")
initial_capital = 100000
equity_curve = (1 + strategy_returns).cumprod() * initial_capital
total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1
print(f"Retorno Total da Estratégia: {total_return:.2%}")
sharpe_ratio = (strategy_returns.mean() / strategy_returns.std()) * np.sqrt(252)
print(f"Índice de Sharpe (anualizado): {sharpe_ratio:.2f}")
rolling_max = equity_curve.cummax()
drawdown = (equity_curve - rolling_max) / rolling_max
max_drawdown = drawdown.min()
print(f"Máximo Drawdown: {max_drawdown:.2%}")

# --- 5. Visualização Avançada ---
buy_and_hold_equity = (1 + asset_returns).cumprod() * initial_capital
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})
ax1.plot(equity_curve.index, equity_curve, label='Estratégia Híbrida Refinada', lw=2)
ax1.plot(buy_and_hold_equity.index, buy_and_hold_equity, label='Buy & Hold', lw=2, linestyle='--')
ax1.set_title('Performance da Estratégia Híbrida Refinada vs. Buy & Hold', fontsize=18)
ax1.set_ylabel('Valor do Portfólio'); ax1.legend(); ax1.grid(True)
ax2.plot(final_position.index, final_position, label='Tamanho da Posição Híbrida', color='purple', alpha=0.7)
ax2.set_title('Tamanho da Posição Diária', fontsize=14)
ax2.set_ylabel('Alavancagem'); ax2.set_xlabel('Data'); ax2.legend(); ax2.grid(True)
plt.tight_layout()
plt.show()