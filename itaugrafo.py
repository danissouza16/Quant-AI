# -*- coding: utf-8 -*-
"""itaugrafo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hj0IeG9GYUiGWiTrsrviwVXipwPD7Gjk
"""

# Célula 1: Instalação e Importação de Bibliotecas

# Instalar bibliotecas, se necessário (geralmente já vêm no Colab)
# !pip install requests pandas beautifulsoup4 lxml html5lib

import requests
import pandas as pd
from bs4 import BeautifulSoup
from io import StringIO # Usado para ler HTML como se fosse arquivo
import datetime as dt
import scipy
import sklearn
import networkx

print("Bibliotecas importadas com sucesso!")

# Célula 2: Definir URL e Carregar Página

# URL da página da Wikipedia com a lista do S&P 500
url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

# Headers para simular um navegador (boa prática para scraping)
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

try:
    response = requests.get(url, headers=headers)
    response.raise_for_status() # Verifica se houve erro no request (ex: 404)
    html_content = response.text
    print(f"Página carregada com sucesso. Tamanho: {len(html_content)} caracteres.")
except requests.exceptions.RequestException as e:
    print(f"Erro ao carregar a página: {e}")
    html_content = None # Sinaliza que o carregamento falhou

# Célula 3 (Corrigida para usar Índice 1 e MultiIndex)

changes_table = None # Inicializa a variável

if html_content:
    try:
        # Usar StringIO para que o pandas leia o HTML da string
        html_file_like = StringIO(html_content)

        # O pandas.read_html retorna uma LISTA de DataFrames
        all_tables = pd.read_html(html_file_like)

        print(f"Encontradas {len(all_tables)} tabelas na página.")

        # --- SELEÇÃO DIRETA DA TABELA DE ALTERAÇÕES (ÍNDICE 1) ---
        if len(all_tables) > 1:
            print("[INFO] Selecionando a Tabela Índice 1 como a tabela de alterações.")
            changes_table = all_tables[1].copy() # Usar .copy() para evitar SettingWithCopyWarning

            # --- TRATAMENTO DO MULTIINDEX ---
            if isinstance(changes_table.columns, pd.MultiIndex):
                print("[INFO] Detectado MultiIndex nas colunas. Planificando...")
                # Concatenar níveis com underscore, tratando possíveis tuplas não esperadas
                changes_table.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in changes_table.columns]
                print("[INFO] Colunas planificadas:", changes_table.columns.tolist())
            else:
                 print("[AVISO] MultiIndex não detectado na Tabela 1, usando colunas como estão:", changes_table.columns.tolist())


            # --- RENOMEAR COLUNAS PARA PADRÃO (AJUSTADO) ---
            # Mapeamento baseado nos nomes planificados observados:
            # 'Effective Date_Effective Date', 'Added_Ticker', 'Added_Security',
            # 'Removed_Ticker', 'Removed_Security', 'Reason_Reason'
            rename_map = {
                'Effective Date_Effective Date': 'Date',
                'Added_Ticker': 'Added Ticker',
                'Added_Security': 'Added Security',
                'Removed_Ticker': 'Removed Ticker',
                'Removed_Security': 'Removed Security',
                'Reason_Reason': 'Reason'
            }

            # Aplicar apenas as renomeações para colunas que existem
            actual_columns = changes_table.columns.tolist()
            rename_map_filtered = {k: v for k, v in rename_map.items() if k in actual_columns}

            changes_table = changes_table.rename(columns=rename_map_filtered)
            print("\n[INFO] Colunas após renomeação (verifique se está correto):", changes_table.columns.tolist())

            print("\nTabela de alterações extraída e colunas ajustadas:")
            print(changes_table.head())

        else:
            print("[ERRO] Número inesperado de tabelas encontradas. Não foi possível isolar a Tabela 1.")

    except Exception as e:
        print(f"Erro ao extrair ou processar a tabela de alterações (Índice 1): {e}")
        changes_table = None # Garante que a variável está None em caso de erro
else:
    print("Conteúdo HTML não carregado, impossível extrair tabela.")
    changes_table = None

# Verificação final
if changes_table is None:
     print("\n[ERRO PÓS-CORREÇÃO] Falha ao obter e processar a tabela de alterações.")

# Célula 4 (Verificar nomes das colunas para limpeza)

if changes_table is not None:
    # 1. Converter coluna 'Date' para datetime (Verificar se 'Date' existe)
    if 'Date' in changes_table.columns:
        try:
            # Tentar converter, tratando múltiplos formatos se necessário
            changes_table['Date'] = changes_table['Date'].astype(str).str.replace('<br>', ' ', regex=False) # Remover <br> se houver
            # Tentar formatos comuns
            changes_table['Date'] = pd.to_datetime(changes_table['Date'], errors='coerce', format='%B %d, %Y')

            # Remover linhas onde a data não pôde ser convertida
            original_rows = len(changes_table)
            changes_table = changes_table.dropna(subset=['Date'])
            if len(changes_table) < original_rows:
                print(f"[AVISO] Removidas {original_rows - len(changes_table)} linhas com datas inválidas.")
            print("[INFO] Coluna 'Date' convertida para datetime.")

        except Exception as e:
            print(f"[ERRO] Erro ao converter coluna 'Date': {e}. Verifique o formato das datas na tabela.")
            # Comprometer o resto do processamento se a data falhar
            changes_table = None # Sinalizar falha
    else:
        print("[ERRO] Coluna 'Date' não encontrada após renomeação na Célula 3. Impossível continuar.")
        changes_table = None # Sinalizar falha

if changes_table is not None: # Prosseguir apenas se a conversão de data funcionou
    # 2. Limpar Tickers (Usar nomes pós-renomeação)
    ticker_cols = ['Added Ticker', 'Removed Ticker']
    for col in ticker_cols:
        if col in changes_table.columns:
            changes_table[col] = changes_table[col].astype(str)
            changes_table[col] = changes_table[col].str.replace(r'\[.*?\]', '', regex=True)
            changes_table[col] = changes_table[col].str.strip()
            changes_table[col] = changes_table[col].replace({'nan': None, '': None, 'None': None, 'NaN':None}) # Adicionado NaN
            print(f"[INFO] Coluna '{col}' limpa.")
        else:
             print(f"[AVISO] Coluna de ticker esperada '{col}' não encontrada.")

    # 3. Ordenar por Data (mais recente primeiro)
    changes_table = changes_table.sort_values(by='Date', ascending=False)
    print("[INFO] Tabela ordenada por data (descendente).")

    # 4. Remover duplicatas
    initial_rows = len(changes_table)
    changes_table = changes_table.drop_duplicates()
    if len(changes_table) < initial_rows:
        print(f"[INFO] Removidas {initial_rows - len(changes_table)} linhas duplicadas.")


    print("\nTabela de alterações após limpeza e ordenação:")
    print(changes_table.head())
    print(f"\nTotal de alterações registradas: {len(changes_table)}")
    if not changes_table.empty:
         print(f"Período coberto pelas alterações: de {changes_table['Date'].min().date()} até {changes_table['Date'].max().date()}")

else:
    print("Tabela de alterações não disponível ou falha na conversão de data.")

# Célula 5: Extrair a Tabela de Constituintes Atuais

current_constituents_list = []
if html_content:
    try:
        html_file_like = StringIO(html_content)
        all_tables = pd.read_html(html_file_like)

        # --- IDENTIFICANDO A TABELA DE CONSTITUINTES ATUAIS ---
        # Heurística: Geralmente é a primeira tabela com colunas como 'Symbol', 'Security', 'GICS Sector'
        constituents_table = None
        for i, table in enumerate(all_tables):
             # Simplificar colunas se for MultiIndex
            flat_columns = []
            if isinstance(table.columns, pd.MultiIndex):
                flat_columns = [col[-1] if isinstance(col, tuple) else col for col in table.columns]
            else:
                flat_columns = table.columns.tolist()

            # Verificar presença de colunas chave (case-insensitive)
            lower_flat_columns = [str(col).lower() for col in flat_columns]
            if 'symbol' in lower_flat_columns and 'security' in lower_flat_columns and 'gics sector' in lower_flat_columns:
                 print(f"Tabela de constituintes atuais encontrada no índice {i}.")
                 constituents_table = table
                 # Corrigir colunas se for MultiIndex
                 if isinstance(constituents_table.columns, pd.MultiIndex):
                     constituents_table.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in constituents_table.columns]

                 # Encontrar a coluna do Ticker (Symbol) - CRÍTICO
                 ticker_col_name = None
                 for col in constituents_table.columns:
                      if 'symbol' in str(col).lower():
                           ticker_col_name = col
                           break

                 if ticker_col_name:
                     current_constituents_list = constituents_table[ticker_col_name].astype(str).str.strip().tolist()
                     print(f"Extraídos {len(current_constituents_list)} constituintes atuais.")
                     # print("Exemplo:", current_constituents_list[:10]) # Descomente para ver exemplo
                 else:
                     print("ERRO: Não foi possível identificar a coluna de Tickers (Symbol) na tabela de constituintes.")

                 break # Parar após encontrar

        if constituents_table is None:
            print("AVISO: Tabela de constituintes atuais não encontrada automaticamente.")

    except Exception as e:
        print(f"Erro ao extrair tabela de constituintes atuais: {e}")
else:
    print("Conteúdo HTML não carregado, impossível extrair constituintes atuais.")

# Verificação final
if not current_constituents_list:
    print("\nERRO CRÍTICO: Não foi possível obter a lista de constituintes atuais. O processo de timeline não pode continuar.")
else:
     # Remover possíveis duplicatas da lista inicial
     current_constituents_list = sorted(list(set(current_constituents_list)))
     print(f"\nLista inicial de constituintes (após remover duplicatas): {len(current_constituents_list)} tickers.")

# Célula 6: Construir a Linha do Tempo Histórica

constituent_timeline = {}

if changes_table is not None and not changes_table.empty and current_constituents_list:
    # A lista atual é válida para "hoje" ou a data mais recente das alterações
    latest_date = changes_table['Date'].max()
    # Usar uma data ligeiramente no futuro para representar o estado "atual"
    current_state_date = pd.Timestamp.now().normalize() + pd.Timedelta(days=1)


    # Usar um set para operações eficientes de adição/remoção
    current_set = set(current_constituents_list)
    constituent_timeline[current_state_date] = sorted(list(current_set)) # Armazenar lista ordenada

    last_processed_date = current_state_date

    # Iterar pelas alterações (já ordenadas da mais recente para a mais antiga)
    for index, row in changes_table.iterrows():
        change_date = row['Date']

        # Se esta data for diferente da última processada, significa que o estado
        # 'current_set' é válido para o período entre change_date (exclusive)
        # e last_processed_date (inclusive). Armazenamos o estado ANTES da mudança.
        if change_date < last_processed_date:
             # Armazena o estado que era válido *antes* desta mudança
             constituent_timeline[last_processed_date] = sorted(list(current_set))


        # Aplicar a mudança atual ao 'current_set' para obter o estado ANTERIOR
        added_ticker = row['Added Ticker']
        removed_ticker = row['Removed Ticker']

        if added_ticker and pd.notna(added_ticker):
             # Se foi adicionado nesta data, NÃO estava antes. Remover.
             if added_ticker in current_set:
                 current_set.remove(added_ticker)
             # else: # Opcional: Log se o ticker adicionado já não estava no set
                 # print(f"Aviso {change_date.date()}: Ticker adicionado '{added_ticker}' não encontrado no set atual para remover.")


        if removed_ticker and pd.notna(removed_ticker):
            # Se foi removido nesta data, ESTAVA antes. Adicionar de volta.
             if removed_ticker not in current_set:
                current_set.add(removed_ticker)
             # else: # Opcional: Log se o ticker removido já estava no set
                # print(f"Aviso {change_date.date()}: Ticker removido '{removed_ticker}' já presente no set ao adicionar de volta.")


        # Atualizar a última data processada
        last_processed_date = change_date

    # Adicionar o estado mais antigo (válido antes da primeira mudança na tabela)
    if not changes_table.empty:
         oldest_change_date = changes_table['Date'].min()
         constituent_timeline[oldest_change_date] = sorted(list(current_set))


    # Opcional: Criar um DataFrame a partir do dicionário para visualização
    timeline_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in constituent_timeline.items() ]))
    # Ordenar colunas (datas)
    timeline_df = timeline_df.reindex(sorted(timeline_df.columns), axis=1)

    print("\nLinha do tempo dos constituintes construída.")
    # print(timeline_df.head()) # Mostra o início do DataFrame da timeline
    print(f"Datas na timeline: {sorted(constituent_timeline.keys())}")


else:
    print("Não foi possível construir a timeline: tabela de alterações ou lista atual de constituintes ausente/vazia.")

# Célula 7: Função para Obter Constituintes em uma Data Específica

def get_constituents_on_date(target_date, timeline):
    """
    Retorna a lista de constituintes do S&P 500 válida na target_date.

    Args:
        target_date (str ou pd.Timestamp): A data desejada.
        timeline (dict): O dicionário da timeline (data -> lista de tickers).

    Returns:
        list: Lista de tickers válidos, ou None se a data for muito antiga
              ou a timeline não estiver disponível.
    """
    if not timeline:
        print("Erro: Timeline não está disponível.")
        return None

    try:
        target_date = pd.to_datetime(target_date)
    except Exception as e:
        print(f"Erro ao converter target_date: {e}")
        return None

    # Encontrar a data na timeline que é a mais recente <= target_date
    valid_dates = sorted([date for date in timeline.keys() if date <= target_date], reverse=True)

    if not valid_dates:
        print(f"Aviso: Nenhuma lista de constituintes encontrada para data <= {target_date.date()}. A data pode ser anterior ao histórico disponível.")
        # Retorna a lista mais antiga disponível como fallback? Ou None? Decidimos por None.
        # oldest_date = min(timeline.keys())
        # return timeline[oldest_date]
        return None
    else:
        # A data correta é a mais recente que é menor ou igual à data alvo
        effective_date = valid_dates[0]
        # print(f"Para {target_date.date()}, usando a lista válida a partir de {effective_date.date()}")
        return timeline[effective_date]

# --- Exemplo de Uso ---
if constituent_timeline:
    test_date = "2023-01-01"
    constituents_on_test_date = get_constituents_on_date(test_date, constituent_timeline)

    if constituents_on_test_date:
        print(f"\nConstituintes em {test_date}: {len(constituents_on_test_date)} tickers.")
        # print(constituents_on_test_date[:20]) # Mostrar os primeiros 20

    # Teste com uma data mais antiga
    old_test_date = "2015-06-15"
    constituents_on_old_date = get_constituents_on_date(old_test_date, constituent_timeline)
    if constituents_on_old_date:
        print(f"\nConstituintes em {old_test_date}: {len(constituents_on_old_date)} tickers.")

    # Teste com data muito antiga (deve retornar None ou aviso)
    very_old_date = "1990-01-01"
    constituents_very_old = get_constituents_on_date(very_old_date, constituent_timeline)
    if constituents_very_old is None:
         print(f"\nComo esperado, não há dados para {very_old_date}.")

# Célula 8: Visualizar Número de Constituintes ao Longo do Tempo

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

if 'constituent_timeline' in locals() and constituent_timeline:
    # Preparar dados para o gráfico
    dates = sorted(constituent_timeline.keys())
    counts = [len(constituent_timeline[date]) for date in dates]

    # Criar DataFrame para facilitar plotagem
    timeline_summary = pd.DataFrame({'Date': dates, 'Constituent Count': counts})

    # Plotar
    plt.figure(figsize=(12, 6))
    plt.plot(timeline_summary['Date'], timeline_summary['Constituent Count'], marker='.', linestyle='-')

    # Formatar eixos
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    plt.gca().xaxis.set_major_locator(mdates.YearLocator(5)) # Marcador a cada 5 anos
    plt.gcf().autofmt_xdate() # Rotacionar datas para melhor leitura

    plt.title('Número Estimado de Constituintes do S&P 500 ao Longo do Tempo (Base Wikipedia)')
    plt.xlabel('Ano')
    plt.ylabel('Número de Constituintes')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.ylim(bottom=max(0, min(counts) - 10), top=max(counts) + 10) # Ajustar limites Y
    plt.tight_layout()
    plt.show()

else:
    print("Erro: 'constituent_timeline' não foi criada ou está vazia.")

# Célula 9: Analisar Frequência e Tipos de Alterações por Ano

if 'changes_table' in locals() and changes_table is not None and not changes_table.empty:
    # Certificar que 'Date' é datetime
    if not pd.api.types.is_datetime64_any_dtype(changes_table['Date']):
         print("Aviso: Convertendo 'Date' para datetime novamente.")
         changes_table['Date'] = pd.to_datetime(changes_table['Date'], errors='coerce')
         changes_table = changes_table.dropna(subset=['Date'])

    # Extrair o ano da data
    changes_table['Year'] = changes_table['Date'].dt.year

    # Contar adições e remoções por ano
    # Considera uma linha como 'adição' se 'Added Ticker' não for nulo
    # Considera uma linha como 'remoção' se 'Removed Ticker' não for nulo
    # Nota: Uma linha pode ter ambos (substituição)
    adds_per_year = changes_table[changes_table['Added Ticker'].notna()].groupby('Year').size()
    removals_per_year = changes_table[changes_table['Removed Ticker'].notna()].groupby('Year').size()

    # Combinar as contagens
    changes_summary = pd.DataFrame({'Adições': adds_per_year, 'Remoções': removals_per_year}).fillna(0).astype(int)
    changes_summary.index.name = 'Ano'

    # Plotar as contagens
    changes_summary.plot(kind='bar', figsize=(14, 7), width=0.8)
    plt.title('Número de Adições e Remoções no S&P 500 por Ano (Base Wikipedia)')
    plt.xlabel('Ano')
    plt.ylabel('Número de Alterações')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()

    # Analisar as razões (Top 10 mais frequentes)
    if 'Reason' in changes_table.columns:
        print("\nTop 10 Razões para Alterações (Frequência):")
        top_reasons = changes_table['Reason'].value_counts().head(10)
        print(top_reasons)
    else:
        print("\nColuna 'Reason' não encontrada para análise.")


else:
    print("Erro: 'changes_table' não foi criada ou está vazia.")

# Célula 10: Verificação Amostral da Consistência dos Tickers

# Instalar yfinance se ainda não estiver instalado
# !pip install yfinance

import yfinance as yf

if 'constituent_timeline' in locals() and constituent_timeline:
    # Datas de amostra (escolha datas relevantes do seu histórico)
    sample_dates = [
        "2024-01-01",
        "2018-06-15",
        "2005-07-01",
        "1998-12-15"
    ]
    # Filtrar datas que estão realmente na timeline para evitar erros
    available_sample_dates = [d for d in sample_dates if pd.to_datetime(d) >= min(constituent_timeline.keys())]

    print(f"Verificando tickers para as datas: {available_sample_dates}")

    problematic_tickers = {} # Dicionário para guardar tickers com problemas {ticker_wiki: erro}
    confirmed_mappings = {} # Dicionário para mapeamentos confirmados {ticker_wiki: ticker_yf}

    for date_str in available_sample_dates:
        print(f"\n--- Verificando data: {date_str} ---")
        wiki_tickers = get_constituents_on_date(date_str, constituent_timeline)

        if not wiki_tickers:
            print(f"Não foi possível obter a lista para {date_str}. Pulando.")
            continue

        print(f"Encontrados {len(wiki_tickers)} tickers na Wikipedia para {date_str}.")

        # Tentar baixar dados de UM dia para verificar a validade dos tickers
        # Usamos 'period="1d"' que pega o dia de negociação mais recente,
        # o suficiente para validar se o ticker existe na API.
        # Poderíamos pegar uma data específica, mas isso é mais simples para validação.
        tickers_string = " ".join(wiki_tickers)
        try:
            # Baixar dados. yfinance lida bem com listas longas.
            # 'actions=False' e 'auto_adjust=False' para acelerar
            data = yf.download(tickers_string, period="1d", actions=False, auto_adjust=False, progress=False)

            # Verificar quais tickers falharam (não aparecem nas colunas do resultado)
            # yfinance retorna MultiIndex columns ('Adj Close', 'Close', etc.) no nível 0
            # e os Tickers no nível 1
            if isinstance(data.columns, pd.MultiIndex):
                valid_yf_tickers = data.columns.get_level_values(1).unique().tolist()
            else:
                 # Se for um único ticker ou ocorrer algo inesperado
                 valid_yf_tickers = data.columns.tolist() if not data.empty else []


            failed_tickers = set(wiki_tickers) - set(valid_yf_tickers)

            if failed_tickers:
                print(f"Tickers da Wikipedia NÃO encontrados diretamente em yfinance:")
                for ticker in sorted(list(failed_tickers)):
                    if ticker not in problematic_tickers: # Registrar apenas a primeira vez
                         problematic_tickers[ticker] = f"Não encontrado em yfinance ({date_str})"
                    print(f" - {ticker}")

                    # --- TENTATIVA DE CORREÇÃO AUTOMÁTICA (Exemplos comuns) ---
                    # Tentar substituir '.' por '-' (ex: BRK.B -> BRK-B)
                    corrected_ticker_dash = ticker.replace('.', '-')
                    if corrected_ticker_dash != ticker:
                        try:
                            # Tentar baixar o ticker corrigido
                            test_corrected = yf.download(corrected_ticker_dash, period="1d", progress=False, actions=False)
                            if not test_corrected.empty:
                                print(f"   -> Encontrado como: {corrected_ticker_dash}")
                                if ticker not in confirmed_mappings:
                                     confirmed_mappings[ticker] = corrected_ticker_dash
                                # Remover da lista de problemáticos se a correção funcionou
                                if ticker in problematic_tickers:
                                    del problematic_tickers[ticker]

                        except Exception:
                             pass # Falha na tentativa de correção

            else:
                print("Todos os tickers da Wikipedia foram encontrados em yfinance para esta data.")


        except Exception as e:
            print(f"Erro GERAL ao tentar baixar dados para a lista de {date_str}: {e}")
            # Marcar todos como potencialmente problemáticos nesta data
            for ticker in wiki_tickers:
                 if ticker not in problematic_tickers:
                     problematic_tickers[ticker] = f"Erro geral no download ({date_str})"


    print("\n--- Resumo da Verificação ---")
    if problematic_tickers:
        print("Tickers problemáticos (precisam de mapeamento manual ou podem não existir em yfinance):")
        for ticker, reason in problematic_tickers.items():
            print(f"- {ticker} ({reason})")
    else:
        print("Nenhum ticker problemático persistente encontrado (após correções automáticas).")

    if confirmed_mappings:
        print("\nMapeamentos sugeridos (Wikipedia -> yfinance):")
        for wiki_ticker, yf_ticker in confirmed_mappings.items():
            print(f"'{wiki_ticker}': '{yf_ticker}',")
    else:
        print("\nNenhum mapeamento automático sugerido.")

else:
    print("Erro: 'constituent_timeline' não disponível para iniciar a verificação.")

# Célula 11: Definir Mapeamento e Atualizar Função de Obtenção de Tickers

# Mapeamento manual (Wikipedia Ticker -> yfinance Ticker)
# Adicione outros mapeamentos aqui se identificar mais problemas
ticker_mapping = {
    'BRK.B': 'BRK-B',
    'BF.B': 'BF-B'
    # Adicione outros mapeamentos necessários aqui. Ex: 'ABC': 'XYZ'
}

print("Mapeamento de Tickers Definido:")
print(ticker_mapping)

def get_mapped_constituents_on_date(target_date, timeline, mapping):
    """
    Obtém a lista de constituintes válida e aplica o mapeamento de tickers.
    Retorna None se a data for inválida ou a timeline não existir.
    """
    wiki_tickers = get_constituents_on_date(target_date, timeline)

    if wiki_tickers is None:
        return None # Data inválida ou timeline ausente

    # Aplicar o mapeamento
    mapped_tickers = [mapping.get(ticker, ticker) for ticker in wiki_tickers]

    # Opcional: Remover duplicatas que podem surgir do mapeamento (raro)
    final_list = sorted(list(set(mapped_tickers)))

    # Opcional: Logar se o tamanho mudou (indicando mapeamento ou remoção de duplicata)
    # if len(final_list) != len(wiki_tickers):
    #     print(f"Info ({pd.to_datetime(target_date).date()}): Tamanho da lista mudou de {len(wiki_tickers)} para {len(final_list)} após mapeamento/dedup.")

    return final_list

# --- Exemplo de Uso da Nova Função ---
if 'constituent_timeline' in locals() and constituent_timeline:
    test_date_mapping = "2024-01-01"
    mapped_list = get_mapped_constituents_on_date(test_date_mapping, constituent_timeline, ticker_mapping)

    if mapped_list:
        print(f"\nExemplo de tickers mapeados para {test_date_mapping}:")
        # Verificar se os tickers mapeados estão na lista
        tickers_to_check = ['BRK-B', 'BF-B']
        found_mapped = [ticker for ticker in tickers_to_check if ticker in mapped_list]
        original_wiki = [wt for wt, yt in ticker_mapping.items() if yt in found_mapped]

        if found_mapped:
             print(f"   Tickers mapeados encontrados na lista: {found_mapped} (originais Wikipedia: {original_wiki})")
        else:
             print(f"   Tickers {tickers_to_check} não encontrados na lista desta data (ou não precisaram de mapeamento).")

        print(f"   Tamanho total da lista mapeada: {len(mapped_list)}")

else:
     print("Timeline não disponível para testar a função de mapeamento.")

# CÉLULA 12 (CORRIGIDA v2): Download Robusto em Lotes
# (Remove o argumento 'raise_errors' inválido)

import yfinance as yf
import time
import pandas as pd
import numpy as np

# --- 1. PARÂMETROS DO DOWNLOAD ---
start_date_prices = "2000-01-01"
end_date_prices = "2024-12-31"
BATCH_SIZE = 50 # Número de tickers para baixar de cada vez
PAUSE_SECONDS = 1 # Segundos de espera entre lotes

# --- 2. COLETA DE TICKERS ÚNICOS (USANDO O MAPEAMENTO) ---
all_unique_tickers = set()
if 'constituent_timeline' in locals() and 'ticker_mapping' in locals():
    timeline_start_date = pd.to_datetime(start_date_prices)
    timeline_end_date = pd.to_datetime(end_date_prices)

    relevant_dates = sorted([d for d in constituent_timeline.keys() if d <= timeline_end_date])

    for date_key in relevant_dates:
        next_date_index = relevant_dates.index(date_key) + 1
        valid_until = relevant_dates[next_date_index] if next_date_index < len(relevant_dates) else timeline_end_date + pd.Timedelta(days=1)

        period_start = max(date_key, timeline_start_date)
        period_end = min(valid_until, timeline_end_date + pd.Timedelta(days=1))

        if period_start < period_end:
            tickers_list = get_mapped_constituents_on_date(date_key, constituent_timeline, ticker_mapping)
            if tickers_list:
                all_unique_tickers.update(tickers_list)

    # ADICIONAR MANUALMENTE O BENCHMARK 'SPY'
    if all_unique_tickers: # Apenas adicione se a coleta foi bem-sucedida
        all_unique_tickers.add('SPY')
        print("Ticker 'SPY' (Benchmark) adicionado manualmente à lista de download.")

    print(f"Total de tickers únicos (mapeados) a serem baixados no período [{start_date_prices} - {end_date_prices}]: {len(all_unique_tickers)}")

else:
    print("ERRO: 'constituent_timeline' ou 'ticker_mapping' não disponíveis.")
    all_unique_tickers = set()

# --- 3. DOWNLOAD ROBUSTO EM LOTES (CHUNKS) ---
all_adj_close_data = [] # Lista para guardar os DataFrames 'Adj Close' de cada lote
failed_tickers_info = {} # Guardar info sobre falhas

if all_unique_tickers:
    tickers_list_download = sorted(list(all_unique_tickers))

    print(f"Iniciando download em {len(tickers_list_download)} tickers em lotes de {BATCH_SIZE}...")

    for i in range(0, len(tickers_list_download), BATCH_SIZE):
        batch_tickers = tickers_list_download[i : i + BATCH_SIZE]
        batch_num = (i // BATCH_SIZE) + 1
        total_batches = (len(tickers_list_download) // BATCH_SIZE) + 1

        print(f"  Baixando lote {batch_num}/{total_batches} ({len(batch_tickers)} tickers)...")

        try:
            batch_data = yf.download(
                tickers = batch_tickers,
                start = start_date_prices,
                end = pd.to_datetime(end_date_prices) + pd.Timedelta(days=1),
                interval = '1d',
                auto_adjust = False,
                actions = False,
                progress = False
                # O argumento 'raise_errors=False' foi removido daqui
            )

            if not batch_data.empty:
                # 1. Pegar apenas 'Adj Close'
                if isinstance(batch_data.columns, pd.MultiIndex):
                    adj_close_batch = batch_data.get('Adj Close')

                    if adj_close_batch is None:
                        # Isso pode acontecer se TODOS no lote falharem
                        print(f"  Lote {batch_num} não retornou 'Adj Close'.")
                        for ticker in batch_tickers:
                            failed_tickers_info[ticker] = "Download do lote falhou (sem Adj Close)"
                        continue # Pular para o próximo lote

                    all_adj_close_data.append(adj_close_batch)

                    # 2. Registrar falhas (tickers que não têm coluna 'Adj Close')
                    downloaded_in_batch = adj_close_batch.columns
                    for ticker in batch_tickers:
                        if ticker not in downloaded_in_batch:
                            failed_tickers_info[ticker] = "No 'Adj Close' data returned (possivelmente delistado)"

                elif len(batch_tickers) == 1:
                     # Se apenas um ticker foi baixado (e foi bem-sucedido)
                     if 'Adj Close' in batch_data.columns:
                         adj_close_batch = batch_data[['Adj Close']]
                         adj_close_batch.columns = [batch_tickers[0]] # Renomear coluna para o ticker
                         all_adj_close_data.append(adj_close_batch)
                     else:
                         failed_tickers_info[batch_tickers[0]] = "No 'Adj Close' data returned"

            else:
                 print(f"  Lote {batch_num} vazio. Todos os tickers falharam.")
                 for ticker in batch_tickers:
                     failed_tickers_info[ticker] = "Download do lote retornou vazio"

            # O PASSO MAIS IMPORTANTE: PAUSA
            print(f"  ...Pausa de {PAUSE_SECONDS} segundo(s) para evitar limite de taxa.")
            time.sleep(PAUSE_SECONDS)

        except Exception as e:
            print(f"  ERRO CRÍTICO ao baixar o lote {batch_num}: {e}")
            for ticker in batch_tickers:
                failed_tickers_info[ticker] = str(e)
            # Pausa extra em caso de erro crítico
            time.sleep(PAUSE_SECONDS * 5)

    # --- 4. CONSOLIDAÇÃO FINAL ---
    print("\nDownload de todos os lotes concluído. Concatenando dados...")

    if all_adj_close_data:
        # Concatenar todos os DataFrames 'Adj Close' pela coluna (axis=1)
        final_adj_close_df = pd.concat(all_adj_close_data, axis=1)

        # Lidar com colunas duplicadas (caso ocorra)
        final_adj_close_df = final_adj_close_df.loc[:, ~final_adj_close_df.columns.duplicated()]

        # Remover colunas que estão totalmente vazias
        final_adj_close_df = final_adj_close_df.dropna(axis=1, how='all')

        # Garantir que o índice é Datetime (necessário para resample, etc.)
        final_adj_close_df.index = pd.to_datetime(final_adj_close_df.index)

        print(f"DataFrame 'final_adj_close_df' criado com {final_adj_close_df.shape[0]} linhas e {final_adj_close_df.shape[1]} colunas.")

        if failed_tickers_info:
            print(f"\n{len(failed_tickers_info)} tickers falharam no download (delistados ou sem dados):")
            # Mostrar os primeiros 20
            for i, (ticker, reason) in enumerate(failed_tickers_info.items()):
                if i >= 20:
                    print(f"... e mais {len(failed_tickers_info) - 20}.")
                    break
                # Mostrar apenas a primeira linha do erro para ser limpo
                print(f"  - {ticker}: {reason.splitlines()[0]}")

        print("\nPrimeiras linhas do DataFrame:")
        print(final_adj_close_df.head())
        print("\nÚltimas linhas do DataFrame:")
        print(final_adj_close_df.tail())

    else:
        print("\nERRO: Nenhum dado foi baixado após todos os lotes.")
        final_adj_close_df = pd.DataFrame() # Definir como vazio

else:
    print("Nenhum ticker único identificado para download.")
    final_adj_close_df = pd.DataFrame() # Definir como vazio

# Célula 13: Calcular Retornos Logarítmicos

import numpy as np

if 'final_adj_close_df' in locals() and not final_adj_close_df.empty:
    print("Calculando retornos logarítmicos diários...")

    # Calcular log returns: log(P_t / P_{t-1})
    log_returns_df = np.log(final_adj_close_df / final_adj_close_df.shift(1))

    # O primeiro dia terá NaN devido ao shift(1)
    log_returns_df = log_returns_df.iloc[1:]

    # Remover colunas que se tornaram totalmente NaN após o cálculo
    # (Pode acontecer se um ticker só tinha 1 dia de dado)
    initial_cols = log_returns_df.shape[1]
    log_returns_df = log_returns_df.dropna(axis=1, how='all')
    if log_returns_df.shape[1] < initial_cols:
        print(f"Removidas {initial_cols - log_returns_df.shape[1]} colunas que ficaram totalmente NaN após cálculo de retorno.")

    print(f"\nDataFrame de Retornos Logarítmicos criado com {log_returns_df.shape[0]} linhas e {log_returns_df.shape[1]} colunas.")
    print("\nPrimeiras linhas:")
    print(log_returns_df.head())
    print("\nÚltimas linhas:")
    print(log_returns_df.tail())

    # Análise rápida de NaNs restantes
    nan_counts = log_returns_df.isna().sum().sort_values(ascending=False)
    print(f"\nNúmero total de células NaN no DataFrame de retornos: {log_returns_df.isna().sum().sum()}")
    print("Tickers com maior número de NaNs (retornos ausentes):")
    print(nan_counts.head(10)) # Mostrar os 10 piores

else:
    print("Erro: DataFrame 'final_adj_close_df' não disponível ou vazio para calcular retornos.")
    log_returns_df = pd.DataFrame() # Criar DataFrame vazio

# Célula 14: Validar Número de Tickers com Dados em Períodos Específicos

if 'log_returns_df' in locals() and not log_returns_df.empty and \
   'constituent_timeline' in locals() and constituent_timeline and \
   'ticker_mapping' in locals():

    # Períodos para validação
    validation_periods = {
        "Ano 2000": ("2000-01-01", "2000-12-31"),
        "Ano 2010": ("2010-01-01", "2010-12-31"),
        "Ano 2012": ("2012-01-01", "2012-12-31"),
        "Ano 2016": ("2016-01-01", "2016-12-31"),
        "Ano 2017": ("2017-01-01", "2017-12-31"),
        "Ano 2018": ("2018-01-01", "2018-12-31"),
        "Ano 2020": ("2020-01-01", "2020-12-31"),
        "Último Ano": ("2024-01-01", "2024-12-31") # Ajuste se end_date_prices for diferente
    }

    print("--- Validação do Número de Tickers Ativos por Período ---")

    for period_name, (start, end) in validation_periods.items():
        start_dt = pd.to_datetime(start)
        end_dt = pd.to_datetime(end)

        # 1. Pegar a lista de constituintes esperados para o início do período
        # Usamos o início do período como referência para a lista S&P
        expected_tickers = get_mapped_constituents_on_date(start_dt, constituent_timeline, ticker_mapping)
        if not expected_tickers:
            print(f"\n{period_name} ({start} a {end}):")
            print("   -> Não foi possível obter a lista de constituintes esperados.")
            continue

        num_expected = len(expected_tickers)

        # 2. Filtrar o DataFrame de retornos para o período
        returns_period = log_returns_df.loc[start_dt:end_dt]

        if returns_period.empty:
            print(f"\n{period_name} ({start} a {end}):")
            print("   -> Nenhum dado de retorno encontrado neste período no DataFrame.")
            print(f"   -> Esperados: {num_expected} tickers.")
            continue

        # 3. Contar quantos tickers tiveram pelo menos UM retorno válido no período
        # Primeiro, selecionar apenas as colunas que estavam na lista esperada
        cols_to_check = [ticker for ticker in expected_tickers if ticker in returns_period.columns]
        returns_period_filtered = returns_period[cols_to_check]

        # Contar colunas que não são inteiramente NaN no período filtrado
        tickers_with_data = returns_period_filtered.dropna(axis=1, how='all').shape[1]

        # 4. Calcular % de cobertura
        coverage_percent = (tickers_with_data / num_expected) * 100 if num_expected > 0 else 0

        print(f"\n{period_name} ({start} a {end}):")
        print(f"   -> Tickers esperados no S&P 500 (início do período): {num_expected}")
        print(f"   -> Tickers com pelo menos 1 retorno válido encontrado: {tickers_with_data}")
        print(f"   -> Cobertura de dados para constituintes: {coverage_percent:.2f}%")

else:
    print("Erro: DataFrame de retornos, timeline ou mapeamento não disponíveis para validação.")

#CELULA 15: FUNÇÕES PRINCIPAIS DA ESTRATEGIA (Versão "MIS de Alto Correlação" + "Ledoit-Wolf")

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.cluster import AgglomerativeClustering
from sklearn.covariance import LedoitWolf
from scipy.optimize import minimize
import networkx as nx
import matplotlib.pyplot as plt

# --- Função Helper para converter Covariância em Correlação ---
def cov_to_corr(cov_matrix):
    """Converte uma matriz de covariância (pd.DataFrame) em uma matriz de correlação."""
    std_devs = np.sqrt(np.diag(cov_matrix.values))
    std_devs[std_devs == 0] = 1.0
    corr_matrix = cov_matrix.div(std_devs, axis=0).div(std_devs, axis=1)
    corr_matrix.values[~np.isfinite(corr_matrix.values)] = 0
    np.fill_diagonal(corr_matrix.values, 1.0)
    return corr_matrix
# ---------------------------------------------------------------

# 1. FUNÇÃO DE CALCULO DE RESIDUOS (Sem alterações)
def get_residual_returns(log_returns_df, benchmark_ticker='SPY'):
    if benchmark_ticker not in log_returns_df.columns:
        raise ValueError(f"Benchmark ticker {benchmark_ticker} não encontrado no DataFrame.")
    benchmark_returns = log_returns_df[benchmark_ticker].values.reshape(-1, 1)
    asset_tickers = log_returns_df.columns.drop(benchmark_ticker, errors='ignore')
    if asset_tickers.empty:
        raise ValueError("Nenhum ativo encontrado no DataFrame (além do benchmark).")
    residual_returns_df = pd.DataFrame(index=log_returns_df.index, columns=asset_tickers, dtype=float)
    for ticker in asset_tickers:
        asset_returns = log_returns_df[ticker]
        valid_indices = ~np.isnan(benchmark_returns.flatten()) & ~np.isnan(asset_returns)
        if np.sum(valid_indices) < 20:
            continue
        y = asset_returns[valid_indices].values.reshape(-1, 1)
        X = benchmark_returns[valid_indices]
        try:
            model = LinearRegression()
            model.fit(X, y)
            predictions = model.predict(X)
            residuals = y - predictions
            residual_series = pd.Series(residuals.flatten(), index=log_returns_df.index[valid_indices])
            residual_returns_df[ticker] = residual_series
        except Exception as e:
            pass
    return residual_returns_df

# 2. FUNÇÃO DE CLUSTERING HIERARQUICO (Corrigida com Ledoit-Wolf)
def get_hierarchical_clusters(residual_returns_df, n_clusters):
    clean_data = residual_returns_df.dropna(axis=1, how='any')
    if clean_data.shape[1] < n_clusters:
        n_clusters = max(1, clean_data.shape[1])
    if clean_data.shape[1] < 2:
         fallback_labels = pd.Series(0, index=residual_returns_df.columns)
         fallback_dist = pd.DataFrame(np.identity(residual_returns_df.shape[1]),
                                      index=residual_returns_df.columns,
                                      columns=residual_returns_df.columns)
         return fallback_labels, fallback_dist
    try:
        estimator = LedoitWolf()
        estimator.fit(clean_data.values)
        cov_matrix_shrunk = pd.DataFrame(estimator.covariance_, index=clean_data.columns, columns=clean_data.columns)
        corr_matrix = cov_to_corr(cov_matrix_shrunk)
    except Exception as e:
        print(f"Erro no Ledoit-Wolf (Clusters), usando Pearson padrão. Erro: {e}")
        corr_matrix = clean_data.corr(method='pearson').fillna(0)
    dist_matrix = np.sqrt(0.5 * (1 - corr_matrix)).fillna(0)
    cluster_model = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric="precomputed",
        linkage='average'
    )
    cluster_labels = cluster_model.fit_predict(dist_matrix)
    ticker_to_cluster = pd.Series(cluster_labels, index=corr_matrix.index)
    ticker_to_cluster = ticker_to_cluster.reindex(residual_returns_df.columns)
    full_dist_matrix = pd.DataFrame(dist_matrix, index=corr_matrix.index, columns=corr_matrix.columns)
    return ticker_to_cluster, full_dist_matrix.reindex(index=residual_returns_df.columns, columns=residual_returns_df.columns)

# CÉLULA 15 (MODIFICADA)

# ... (todas as outras funções - cov_to_corr, get_residual_returns, get_hierarchical_clusters - permanecem iguais) ...

# 3. FUNÇÃO DE SELEÇÃO DE CLUSTERS (MODIFICADA PARA Z-Score)
def get_selected_clusters_mis(residual_returns_df, ticker_to_cluster, Z_threshold):
    # --- MUDANÇA: A assinatura da função agora recebe Z_threshold ---

    n_clusters = int(ticker_to_cluster.nunique())
    if pd.isna(ticker_to_cluster).any():
        n_clusters = int(ticker_to_cluster.dropna().nunique())

    if n_clusters <= 1:
        return [0], nx.Graph()

    clean_data = residual_returns_df.dropna(axis=1, how='any')
    if clean_data.shape[1] < 2:
        return [0], nx.Graph()

    try:
        estimator = LedoitWolf()
        estimator.fit(clean_data.values)
        cov_matrix_shrunk = pd.DataFrame(estimator.covariance_, index=clean_data.columns, columns=clean_data.columns)
        full_corr_matrix = cov_to_corr(cov_matrix_shrunk)
    except Exception as e:
        print(f"Erro no Ledoit-Wolf (MIS), usando Pearson. Erro: {e}")
        full_corr_matrix = clean_data.corr(method='pearson').fillna(0)

    inter_cluster_corr = pd.DataFrame(index=range(n_clusters), columns=range(n_clusters), dtype=float)

    for i in range(n_clusters):
        for j in range(i, n_clusters):
            tickers_i = ticker_to_cluster[ticker_to_cluster == i].index
            tickers_j = ticker_to_cluster[ticker_to_cluster == j].index

            tickers_i = [t for t in tickers_i if t in full_corr_matrix.index]
            tickers_j = [t for t in tickers_j if t in full_corr_matrix.columns]

            if i == j:
                inter_cluster_corr.iloc[i, j] = 1.0
            elif not tickers_i or not tickers_j:
                inter_cluster_corr.iloc[i, j] = np.nan
                inter_cluster_corr.iloc[j, i] = np.nan
            else:
                cross_corr_subset = full_corr_matrix.loc[tickers_i, tickers_j]
                mean_corr = np.median(np.abs(cross_corr_subset.values)) # Usar mediana é mais robusto
                inter_cluster_corr.iloc[i, j] = mean_corr
                inter_cluster_corr.iloc[j, i] = mean_corr

    upper_triangle = inter_cluster_corr.values[np.triu_indices_from(inter_cluster_corr.values, k=1)]
    upper_triangle = upper_triangle[~np.isnan(upper_triangle)]

    if len(upper_triangle) == 0:
        threshold = 1.0 # Se não houver dados, não crie arestas de conflito
    else:
        # --- MUDANÇA: Lógica do limiar (threshold) ---
        # Substituímos o percentil por um limiar estatístico baseado em Z-score
        corr_mean = np.mean(upper_triangle)
        corr_std = np.std(upper_triangle)
        threshold = corr_mean + Z_threshold * corr_std
        # --- FIM DA MUDANÇA ---

    # Construir o Grafo de CONFLITO
    G = nx.Graph()
    for i in range(n_clusters):
        G.add_node(i)

    try:
        for i in range(n_clusters):
            for j in range(i + 1, n_clusters):
                corr_val = inter_cluster_corr.iloc[i, j]
                # Aresta = ALTA correlação (conflito)
                if pd.notna(corr_val) and corr_val > threshold:
                    G.add_edge(i, j)

        # Selecionar o CONJUNTO INDEPENDENTE (MIS)
        selected_clusters = list(nx.maximal_independent_set(G))
        if not selected_clusters:
            selected_clusters = [0] # Fallback

    except Exception as e:
        selected_clusters = [0]

    return selected_clusters, G

# 4. FUNÇÃO DE SELEÇÃO DE ATIVOS (INTRA-CLUSTER) (Robusta para NaNs)
def select_assets_from_clusters(selected_clusters, ticker_to_cluster, dist_matrix, max_assets_per_cluster):
    final_asset_list = []
    dist_matrix = dist_matrix.fillna(np.inf)

    if max_assets_per_cluster == 'all':
        for cluster_id in selected_clusters:
            tickers_in_cluster = ticker_to_cluster[ticker_to_cluster == cluster_id].index
            valid_tickers = [t for t in tickers_in_cluster if t in dist_matrix.index]
            final_asset_list.extend(valid_tickers)
        return final_asset_list

    for cluster_id in selected_clusters:
        tickers_in_cluster = ticker_to_cluster[ticker_to_cluster == cluster_id].index
        valid_tickers = [t for t in tickers_in_cluster if t in dist_matrix.index]
        if not valid_tickers:
            continue
        if len(valid_tickers) <= max_assets_per_cluster:
            final_asset_list.extend(valid_tickers)
        else:
            cluster_dist_matrix = dist_matrix.loc[valid_tickers, valid_tickers]
            centrality = cluster_dist_matrix.sum(axis=1)
            selected_assets = centrality.nsmallest(max_assets_per_cluster).index.tolist()
            final_asset_list.extend(selected_assets)

    return final_asset_list

# 5. FUNÇÃO DE ALOCAÇÃO DE PESOS (OTIMIZAÇÃO) (Robusta para NaNs)
def get_min_variance_allocation(asset_list, log_returns_df, max_weight_per_asset,
                                turnover_cost_bps, prev_weights):
    if not asset_list:
        return pd.Series(dtype=float)
    valid_asset_list = [asset for asset in asset_list if asset in log_returns_df.columns]
    if not valid_asset_list:
        return pd.Series(dtype=float)
    lookback_data = log_returns_df[valid_asset_list].dropna(axis=1, how='any')
    final_asset_list = lookback_data.columns.tolist()
    if not final_asset_list or len(final_asset_list) < 2:
         if len(final_asset_list) == 1:
             return pd.Series(1.0, index=final_asset_list)
         return pd.Series(dtype=float)
    try:
        estimator = LedoitWolf()
        estimator.fit(lookback_data.values)
        cov_matrix = pd.DataFrame(estimator.covariance_, index=final_asset_list, columns=final_asset_list)
        cov_matrix = cov_matrix * 252
    except Exception as e:
        cov_matrix = lookback_data.cov() * 252
        if cov_matrix.empty or cov_matrix.isnull().all().all():
             return pd.Series(1/n_assets, index=final_asset_list)
    n_assets = len(final_asset_list)
    def objective_function(w, cov_matrix, turnover_cost, w_prev):
        portfolio_variance = w.T @ cov_matrix @ w
        turnover_penalty = turnover_cost * np.sum(np.abs(w - w_prev))
        return portfolio_variance + turnover_penalty
    constraints = [
        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},
    ]
    bounds = tuple((0, max_weight_per_asset) for _ in range(n_assets))
    w_init = np.array([1/n_assets] * n_assets)
    w_prev_aligned = prev_weights.reindex(final_asset_list).fillna(0.0).values
    turnover_cost_decimal = turnover_cost_bps / 10000.
    try:
        opt_result = minimize(
            objective_function,
            w_init,
            args=(cov_matrix, turnover_cost_decimal, w_prev_aligned),
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            tol=1e-9
        )
        if opt_result.success:
            final_weights = pd.Series(opt_result.x, index=final_asset_list)
            final_weights[final_weights < 1e-8] = 0
            return final_weights / final_weights.sum()
        else:
            return pd.Series(1/n_assets, index=final_asset_list)
    except Exception as e:
        return pd.Series(1/n_assets, index=final_asset_list)

print("Célula 15 (Versão MIS Z-Score): Funções da Estratégia Híbrida definidas.")

# CÉLULA 15.A

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold

# --- Função Helper (Faltante) ---
def get_train_times(t1_train: pd.Series, t_test: pd.Series) -> pd.Series:
    """
    Função helper (baseada em de Prado) para purgar o conjunto de treino.
    Remove qualquer evento de treino (t1_train) que sobreponha um evento de teste (t_test).

    Args:
        t1_train: pd.Series onde o índice é o início do evento de treino,
                  e o valor é o fim do evento.
        t_test: pd.Series com uma única linha, onde o índice é o início
                do período de teste e o valor é o fim.

    Returns:
        pd.Series: Eventos de treino (t1_train) que não sobrepõem o teste.
    """
    t_test_start = t_test.index[0]
    t_test_end = t_test.iloc[0]

    # Identificar eventos de treino que devem ser purgados
    # 1. Fim do treino ocorre durante o teste
    purge_mask1 = (t1_train >= t_test_start) & (t1_train <= t_test_end)

    # 2. Início do treino ocorre durante o teste
    purge_mask2 = (t1_train.index >= t_test_start) & (t1_train.index <= t_test_end)

    # 3. Treino "envelopa" o teste
    purge_mask3 = (t1_train.index <= t_test_start) & (t1_train >= t_test_end)

    # Combina todas as condições de purga
    purge_mask_all = purge_mask1 | purge_mask2 | purge_mask3

    # Retorna apenas os eventos de treino que NÃO devem ser purgados
    return t1_train[~purge_mask_all]

# --- Classe PurgedKFold (Seu Código) ---
class PurgedKFold(KFold):
    """
    Modifica o KFold para séries temporais financeiras para prevenir "leakage".
    Baseado em Lopez de Prado, Advances in Financial Machine Learning, Cap. 7.
    Remove do conjunto de treino:
    1. Observações que sobrepõem o período de teste (Purging).
    2. Observações que ocorrem logo após o teste (Embargo).
    """
    def __init__(self, n_splits=10, t1=None, pct_embargo=0.01):
        if not isinstance(t1, pd.Series):
            raise ValueError("t1 deve ser uma pd.Series contendo os tempos de término dos eventos.")
        # Forçamos shuffle=False para manter a ordem temporal
        super(PurgedKFold, self).__init__(n_splits, shuffle=False)
        self.t1 = t1
        self.pct_embargo = pct_embargo

    def split(self, X, y=None, groups=None):
        if (X.index == self.t1.index).sum() != len(self.t1):
            raise ValueError("X e t1 devem ter o mesmo índice.")

        indices = np.arange(X.shape[0])
        # Calcula o tamanho do embargo em número de amostras
        embargo_size = int(X.shape[0] * self.pct_embargo)

        # O KFold padrão (sem shuffle) divide sequencialmente (walk-forward)
        # Usamos array_split para obter os pontos de início dos folds
        test_starts_indices = [i[0] for i in np.array_split(indices, self.n_splits)]

        for i in range(self.n_splits):
            # --- 1. Definir o conjunto de teste (Walk-Forward) ---
            train_start_idx = 0
            test_start_idx = test_starts_indices[i]

            # Não podemos testar no primeiro fold, pois não há treino
            if test_start_idx == 0:
                continue

            train_end_idx = test_start_idx - 1

            if i < self.n_splits - 1:
                test_end_idx = test_starts_indices[i+1] - 1
            else:
                test_end_idx = X.shape[0] - 1 # Último fold vai até o fim

            train_indices = indices[train_start_idx : train_end_idx + 1]
            test_indices = indices[test_start_idx : test_end_idx + 1]

            if len(test_indices) == 0:
                continue

            # --- 2. Purgar o conjunto de treino ---
            # t1 (pd.Series) mapeia o índice (data de início) para a data de fim do evento

            # Tempo de início do teste
            t0_test = self.t1.index[test_start_idx]
            # Tempo de fim do último evento no teste
            t1_test_max = self.t1.iloc[test_indices].max()

            # Criar a Series test_times para a função de purga
            test_times = pd.Series([t1_test_max], index=[t0_test])

            # Obter os tempos de treino (índice) que NÃO sobrepõem os tempos de teste
            train_times_purged = get_train_times(self.t1.iloc[train_indices], test_times)

            # Converter os tempos de treino (índice) de volta para posições (inteiros)
            train_indices = self.t1.index.searchsorted(train_times_purged.index)

            # --- 3. Embargar o conjunto de treino ---
            # Removemos 'embargo_size' amostras *antes* do início do teste
            # para evitar autocorrelação de curto prazo
            if train_indices.shape[0] > 0:
                embargo_start_idx_in_train = max(0, train_indices.shape[0] - embargo_size)
                train_indices_final = train_indices[:embargo_start_idx_in_train]
            else:
                train_indices_final = train_indices

            yield train_indices_final, test_indices

print("Célula 15.A: Funções 'get_train_times' e classe 'PurgedKFold' definidas.")

#CELULA 15.B: Instalação da Otimização Bayesiana

print("Instalando scikit-optimize...")
!pip install scikit-optimize -q

import skopt
from skopt import gp_minimize
from skopt.space import Integer, Categorical
from skopt.utils import use_named_args
from skopt.plots import plot_convergence

print("scikit-optimize (skopt) importado com sucesso.")

# CÉLULA 15.C (NOVA): Análise de Qualidade de Cluster (Pré-Calibração)
# Objetivo: Encontrar um range justificado para o HP 'n_clusters'

import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

print("Iniciando Análise de Qualidade de Cluster (Pré-calibração)...")

# --- Carregar Dados de Análise ---
# Usar os mesmos dados que serão usados na calibração para consistência
# (Baseado na Célula 16 original)
ROBUST_START_DATE = "2017-01-01"
BENCHMARK = 'SPY'

# Carregar dados de log_returns (assumindo que 'log_returns_df' está em memória)
try:
    log_returns_analysis = log_returns_df.loc[ROBUST_START_DATE:].dropna(axis=1, how="all")

    # 1. Calcular Resíduos (usando a função da Célula 15)
    print(f"Calculando resíduos contra {BENCHMARK}...")
    residual_returns = get_residual_returns(log_returns_analysis, benchmark_ticker=BENCHMARK)

    # Usar apenas ativos com resíduos válidos para a análise de cluster
    clean_data_resid = residual_returns.dropna(axis=1, how='any')

    if clean_data_resid.shape[1] < 50: # Garantir que temos dados suficientes
        print("[AVISO] Dados de resíduos limpos têm menos de 50 ativos. A análise pode ser instável.")

except NameError:
    print("[ERRO] Variável 'log_returns_df' não encontrada. Execute as células anteriores primeiro.")
except Exception as e:
    print(f"[ERRO] Falha ao preparar dados: {e}")


# --- Loop de Teste para n_clusters ---
cluster_range = range(10, 51, 2) # Testar de 10 a 50, em passos de 2
silhouette_scores = []
n_clusters_tested = []

print(f"Testando n_clusters de {min(cluster_range)} a {max(cluster_range)}...")

for n in cluster_range:
    if n > clean_data_resid.shape[1]:
        print(f"Pulando n={n}, maior que o número de ativos válidos ({clean_data_resid.shape[1]}).")
        break

    try:
        # 2. Rodar o clustering (usando a função da Célula 15)
        ticker_to_cluster, dist_matrix = get_hierarchical_clusters(
            clean_data_resid,
            n_clusters=n
        )

        # Preparar labels para o silhouette score
        # (Apenas para os ativos que realmente foram clusterizados)
        valid_labels = ticker_to_cluster.loc[clean_data_resid.columns]

        # 3. Calcular Métrica de Silhueta
        # Nota: Usamos 'clean_data_resid.T' porque o silhouette score espera (n_samples, n_features).
        # No nosso caso, os "samples" são os *ativos* (transpostos).
        score = silhouette_score(clean_data_resid.T, valid_labels, metric='correlation')

        silhouette_scores.append(score)
        n_clusters_tested.append(n)

    except Exception as e:
        print(f"Falha ao calcular para n_clusters={n}. Erro: {e}")

# --- Plotar Resultados ---
if n_clusters_tested:
    print("Análise de Qualidade de Cluster concluída.")

    plt.figure(figsize=(10, 6))
    plt.plot(n_clusters_tested, silhouette_scores, marker='o', linestyle='--')
    plt.title('Análise de Qualidade de Cluster (Métrica de Silhueta)')
    plt.xlabel('Número de Clusters (n_clusters)')
    plt.ylabel('Silhouette Score (Maior é Melhor)')
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.show()

    # --- Recomendação ---
    best_n_index = np.argmax(silhouette_scores)
    best_n = n_clusters_tested[best_n_index]
    print(f"\nO melhor Silhouette Score foi {silhouette_scores[best_n_index]:.4f} com n_clusters = {best_n}.")
    print(f"Recomendação: Use um range para 'n_clusters' centrado em torno deste valor na Célula 16 (ex: Integer({max(10, best_n-5)}, {best_n+5})).")
else:
    print("[ERRO] Nenhuma análise de cluster pôde ser concluída.")

# CÉLULA 15.D (NOVA): Diagnóstico Visual do Grafo de Clusters

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

print("Iniciando Diagnóstico Visual do Grafo de Clusters...")

# --- Parâmetros de Teste ---
# Vamos usar os parâmetros que encontramos como sendo os mais robustos
N_CLUSTERS_ROBUSTO = 18  # Vindo da sua análise da Célula 15.C
Z_THRESHOLD_EXEMPLO = 1.0 # Um Z-score de 1.0 como exemplo
BENCHMARK = 'SPY'

# --- Carregar Dados de Análise ---
# Usar os mesmos dados da Célula 15.C para consistência
try:
    # 1. Calcular Resíduos
    print(f"Calculando resíduos contra {BENCHMARK}...")
    residual_returns = get_residual_returns(log_returns_analysis, benchmark_ticker=BENCHMARK)
    clean_data_resid = residual_returns.dropna(axis=1, how='any')

    # 2. Rodar o clustering
    print(f"Executando clustering para n_clusters = {N_CLUSTERS_ROBUSTO}...")
    ticker_to_cluster, dist_matrix = get_hierarchical_clusters(
        clean_data_resid,
        n_clusters=N_CLUSTERS_ROBUSTO
    )

    # 3. Calcular a Matriz de Correlação Inter-Cluster
    # (Esta é a lógica de dentro da 'get_selected_clusters_mis')
    print("Calculando matriz de correlação inter-cluster...")

    # Re-calcular a matriz de correlação (Ledoit-Wolf) sobre os dados limpos
    estimator = LedoitWolf()
    estimator.fit(clean_data_resid.values)
    cov_matrix_shrunk = pd.DataFrame(estimator.covariance_, index=clean_data_resid.columns, columns=clean_data_resid.columns)
    full_corr_matrix = cov_to_corr(cov_matrix_shrunk)

    inter_cluster_corr = pd.DataFrame(index=range(N_CLUSTERS_ROBUSTO), columns=range(N_CLUSTERS_ROBUSTO), dtype=float)

    for i in range(N_CLUSTERS_ROBUSTO):
        for j in range(i, N_CLUSTERS_ROBUSTO):
            tickers_i = ticker_to_cluster[ticker_to_cluster == i].index
            tickers_j = ticker_to_cluster[ticker_to_cluster == j].index

            tickers_i = [t for t in tickers_i if t in full_corr_matrix.index]
            tickers_j = [t for t in tickers_j if t in full_corr_matrix.columns]

            if i == j:
                inter_cluster_corr.iloc[i, j] = 1.0
            elif not tickers_i or not tickers_j:
                inter_cluster_corr.iloc[i, j] = np.nan
                inter_cluster_corr.iloc[j, i] = np.nan
            else:
                cross_corr_subset = full_corr_matrix.loc[tickers_i, tickers_j]
                mean_corr = np.median(np.abs(cross_corr_subset.values)) # Usando mediana
                inter_cluster_corr.iloc[i, j] = mean_corr
                inter_cluster_corr.iloc[j, i] = mean_corr

    # 4. Calcular o Limiar (Threshold)
    upper_triangle = inter_cluster_corr.values[np.triu_indices_from(inter_cluster_corr.values, k=1)]
    upper_triangle = upper_triangle[~np.isnan(upper_triangle)]

    corr_mean = np.mean(upper_triangle)
    corr_std = np.std(upper_triangle)
    threshold = corr_mean + Z_THRESHOLD_EXEMPLO * corr_std

    print(f"\n--- Estatísticas do Grafo (para Z={Z_THRESHOLD_EXEMPLO}) ---")
    print(f"Média da Correlação Inter-Cluster (μ): {corr_mean:.4f}")
    print(f"Desv. Padrão da Correlação (σ): {corr_std:.4f}")
    print(f"Limiar de Conflito (μ + Z*σ): {threshold:.4f}")

    # 5. Plotar os Gráficos de Diagnóstico
    fig, axes = plt.subplots(1, 2, figsize=(18, 7))

    # Gráfico 1: Heatmap da Matriz de Correlação
    sns.heatmap(inter_cluster_corr, ax=axes[0], cmap='viridis', annot=False)
    axes[0].set_title(f'Heatmap de Correlação Inter-Cluster (n={N_CLUSTERS_ROBUSTO})')
    axes[0].set_xlabel('Cluster ID')
    axes[0].set_ylabel('Cluster ID')

    # Gráfico 2: Distribuição das Correlações
    sns.histplot(upper_triangle, ax=axes[1], kde=True)
    axes[1].axvline(corr_mean, color='orange', linestyle='--', label=f'Média (μ) = {corr_mean:.4f}')
    axes[1].axvline(threshold, color='red', linestyle='-', label=f'Limiar (Threshold) = {threshold:.4f}')
    axes[1].set_title('Distribuição das Correlações Inter-Cluster')
    axes[1].set_xlabel('Correlação')
    axes[1].set_ylabel('Contagem')
    axes[1].legend()

    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"[ERRO] Falha ao executar célula de diagnóstico: {e}")
    print("Certifique-se de que as células 15 e 15.C foram executadas e 'log_returns_analysis' existe.")

# CÉLULA 16 (MODIFICADA - ITERAÇÃO 3: Foco na Estabilidade)

import warnings
from skopt import gp_minimize
from skopt.space import Integer, Categorical, Real
from skopt.utils import use_named_args
from skopt.plots import plot_convergence

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

print("Iniciando Célula 16: Otimização Bayesiana (Foco na Estabilidade)...")

# --- MUDANÇA: HPs Fixos ---
# Fixamos os HPs ruidosos ou com hipótese clara
FIXED_LOOKBACK_WINDOW = 500 # Valor defensável (aprox. 2 anos)
FIXED_MAX_ASSETS = 'all'      # Testando a hipótese de diversificação intra-cluster

# 1. DEFINIR O ESPAÇO DE BUSCA (REDUZIDO)
# Focando apenas nos HPs que definem a mecânica do grafo e o turnover
search_space = [
    # n_clusters: Manter o range robusto da análise de Silhueta
    Integer(13, 23, name='n_clusters'),

    # Z_threshold: Manter o range estatístico amplo
    Real(0.5, 3.0, name='Z_threshold'),

    # turnover_cost_bps: Manter o range econômico
    Integer(0, 20, name='turnover_cost_bps')
]

# 2. CARREGAR DADOS DE CV E FOLDS
try:
    ROBUST_START_DATE = "2017-01-01"
    FORWARD_WINDOW = 63

    # --- MUDANÇA: Aumentar o número de Folds para estabilizar o score ---
    N_SPLITS_CV = 10

    log_returns_cv_full = log_returns_df.loc[ROBUST_START_DATE:].dropna(axis=1, how="all")
    t1_series = pd.Series(log_returns_cv_full.index, index=log_returns_cv_full.index).shift(-FORWARD_WINDOW)
    t1_series = t1_series.dropna()

    log_returns_cv = log_returns_cv_full.loc[t1_series.index]

    print(f"Dados de CV (alinhados com labels): {log_returns_cv.shape[0]} dias, {log_returns_cv.shape[1]} ativos.")
    print(f"Preparando {N_SPLITS_CV} folds com PurgedKFold...")

    kf = PurgedKFold(n_splits=N_SPLITS_CV, t1=t1_series, pct_embargo=0.01)
    cv_indices = list(kf.split(log_returns_cv))
    print(f"Dados e {len(cv_indices)} Folds carregados com sucesso.")
except Exception as e:
    print(f"[ERRO CRÍTICO] ao preparar dados de CV: {e}")
    raise

# 3. DEFINIR A FUNÇÃO OBJETIVO (MODIFICADA)
@use_named_args(search_space)
def objective_function(n_clusters, Z_threshold, turnover_cost_bps):

    # Usar HPs fixos
    LOOKBACK_WINDOW = FIXED_LOOKBACK_WINDOW
    max_assets_per_cluster = FIXED_MAX_ASSETS

    fold_oos_variances = []

    for fold_idx, (train_idx, test_idx) in enumerate(cv_indices):
        if len(train_idx) == 0 or len(test_idx) == 0:
            continue

        calibration_point_idx = train_idx[-1]
        if calibration_point_idx < LOOKBACK_WINDOW:
            continue

        lookback_start_idx = max(0, calibration_point_idx - LOOKBACK_WINDOW)
        lookback_data = log_returns_cv.iloc[lookback_start_idx : calibration_point_idx + 1]
        oos_data = log_returns_cv.iloc[test_idx]

        if lookback_data.empty or oos_data.empty:
            continue

        try:
            residuals = get_residual_returns(lookback_data, benchmark_ticker="SPY")
            valid_assets_resid = residuals.dropna(axis=1, how='any')

            if valid_assets_resid.shape[1] < max(2, n_clusters):
                raise ValueError("Sem ativos suficientes após cálculo de resíduos")

            ticker_to_cluster, dist_matrix = get_hierarchical_clusters(
                valid_assets_resid, n_clusters=n_clusters
            )

            selected_clusters, _ = get_selected_clusters_mis(
                residuals, ticker_to_cluster, Z_threshold=Z_threshold
            )

            if not selected_clusters:
                raise ValueError("Nenhum cluster selecionado pelo MIS")

            final_assets = select_assets_from_clusters(
                selected_clusters,
                ticker_to_cluster,
                dist_matrix,
                max_assets_per_cluster=max_assets_per_cluster
            )

            if not final_assets:
                raise ValueError("Nenhum ativo final selecionado")

            weights = get_min_variance_allocation(
                final_assets,
                lookback_data,
                0.10, # max_weight_per_asset (fixo em 10%)
                turnover_cost_bps, # HP vindo da otimização
                pd.Series(dtype=float) # prev_weights (não usado no CV)
            )

            if weights.empty:
                raise ValueError("Otimização não retornou pesos.")

            oos_returns_assets = oos_data.reindex(columns=weights.index)
            oos_portfolio_returns = (oos_returns_assets.fillna(0) @ weights).values

            if len(oos_portfolio_returns) < 2:
                raise ValueError("Não há retornos OOS suficientes para calcular a variância.")

            oos_variance = np.var(oos_portfolio_returns)
            fold_oos_variances.append(oos_variance)

        except Exception as e:
            fold_oos_variances.append(np.inf)

    mean_oos_variance = np.mean([v for v in fold_oos_variances if np.isfinite(v)])
    if not np.isfinite(mean_oos_variance):
        mean_oos_variance = np.inf

    return mean_oos_variance

# 4. EXECUTAR A OTIMIZAÇÃO BAYESIANA
# --- MUDANÇA: Mais trials e pontos iniciais ---
N_TRIALS = 75
N_INITIAL_POINTS = 15

if 'cv_indices' in locals():
    print(f"Iniciando Otimização Bayesiana (gp_minimize) para {N_TRIALS} iterações...")

    opt_result = gp_minimize(
        func=objective_function,
        dimensions=search_space,
        n_calls=N_TRIALS,
        n_initial_points=N_INITIAL_POINTS,
        acq_func="EI",
        random_state=42
    )

    # 5. OBTER OS MELHORES PARÂMETROS
    print("\n--- CALIBRAÇÃO CONCLUÍDA ---")
    print(f"Melhor score (Min OOS Var): {opt_result.fun:.10f}")

    # --- MUDANÇA: Dicionário de 'best_params' atualizado ---
    best_params = {
        'LOOKBACK_WINDOW': FIXED_LOOKBACK_WINDOW, # Valor Fixo
        'n_clusters': opt_result.x[0],
        'Z_threshold': opt_result.x[1],
        'max_assets_per_cluster': FIXED_MAX_ASSETS, # Valor Fixo
        'turnover_cost_bps': opt_result.x[2]
    }

    print(f"Melhores Parâmetros: {best_params}")

    print("\nGráfico de Convergência da Otimização:")
    plot_convergence(opt_result)
    plt.show()
else:
    print("[ERRO] Dados de CV (cv_indices) não foram criados. Otimização não pode ser executada.")

#CELULA 16.B: ANALISE DE SENSIBILIDADE (VERSÃO VISUAL LIMPA - APENAS PONTOS)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Importar as funções de plotagem do skopt (apenas para convergência)
try:
    from skopt.plots import plot_convergence
except ImportError:
    print("Execute 'pip install scikit-optimize' para instalar as bibliotecas de plotagem.")

print("Iniciando Célula 16.B: Análise de Sensibilidade da Calibração...")

try:
    # 1. Verificar se os resultados da Célula 16 existem
    if 'opt_result' not in locals() or 'search_space' not in locals():
        raise NameError("ERRO: 'opt_result' ou 'search_space' (da Célula 16) não encontrados. Rode a Célula 16 primeiro.")

    # 2. Extrair os dados da Otimização Bayesiana
    param_names = [dim.name for dim in search_space]
    scores = opt_result.func_vals
    params_tested = opt_result.x_iters

    # 3. Criar um DataFrame para facilitar a plotagem
    results_df_skopt = pd.DataFrame(params_tested, columns=param_names)
    results_df_skopt['mean_oos_variance'] = scores

    # Remover quaisquer testes que falharam (score = inf) ou NaNs
    results_df_skopt.replace([np.inf, -np.inf], np.nan, inplace=True)
    results_df_skopt.dropna(inplace=True)

    if len(results_df_skopt) == 0:
        raise ValueError("Nenhum resultado de calibração válido foi encontrado.")

    print(f"Analisando {len(results_df_skopt)} testes de calibração bem-sucedidos...")

    # Melhor ponto
    best_params = {param_names[i]: opt_result.x[i] for i in range(len(param_names))}
    best_score = opt_result.fun

    # ---
    # ANÁLISE TEXTUAL (TERMINAL)
    # ---
    print("\n" + "="*50)
    print("--- 1. MELHOR RESULTADO ENCONTRADO ---")
    print(f"Score (Variância Média OOS): {best_score:.10f}")
    print("Melhores Hiperparâmetros:")
    for param_name, value in best_params.items():
        print(f"  - {param_name}: {value}")

    print("\n--- 2. ANÁLISE DE SENSIBILIDADE 'k' (max_assets_per_cluster) ---")
    if 'max_assets_per_cluster' in results_df_skopt.columns:
        k_analysis = results_df_skopt.groupby('max_assets_per_cluster')['mean_oos_variance'].agg(['mean', 'std', 'count'])
        if 'all' in k_analysis.index:
             k_analysis_num = k_analysis.drop('all')
             k_analysis_all = k_analysis.loc[['all']]
             k_analysis = pd.concat([k_analysis_num.sort_index(), k_analysis_all])

        print("Score (Variância OOS) Médio por 'k': (Menor é Melhor)")
        print(k_analysis.to_string(float_format='{:.8f}'.format))
    else:
        print("Não foi possível analisar 'max_assets_per_cluster'.")

    print("\n--- 3. TOP 5 MELHORES COMBINAÇÕES ENCONTRADAS ---")
    top_5_runs = results_df_skopt.sort_values(by='mean_oos_variance').head(5)
    print(top_5_runs.to_string(float_format='{:.8f}'.format))
    print("="*50 + "\n")
    # --- FIM DA ANÁLISE TEXTUAL ---


    # ---
    # GRÁFICO 1: ANÁLISE DE SENSIBILIDADE 1D (COM CURVAS DE REGRESSÃO)
    # ---
    print("\n--- Gráfico 1: Análise de Sensibilidade 1D (HP vs Score com Curvas) ---")
    n_params = len(param_names)
    n_cols = 2
    n_rows = int(np.ceil(n_params / n_cols))

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 6))
    axes = axes.flatten()

    for i, param_name in enumerate(param_names):
        ax = axes[i]
        best_param_value = best_params[param_name]

        # <--- MUDANÇA: Gráficos 1D são agora apenas de DISPERSÃO

        if isinstance(search_space[i], Categorical):
            # Para categóricos, usamos stripplot
            categories_order = search_space[i].categories
            sns.stripplot(x=param_name, y='mean_oos_variance', data=results_df_skopt, ax=ax,
                          order=categories_order, jitter=0.2, alpha=0.5, color='gray', label='Testes')
            ax.set_title(f'Distribuição (Categórica): {param_name} vs. Score')

            try:
                best_param_x_pos = categories_order.index(best_param_value)
                ax.scatter(x=[best_param_x_pos], y=[best_score],
                           color='red', marker='X', s=200, zorder=10,
                           edgecolor='black', linewidth=1.5,
                           label=f"Melhor (Score: {best_score:.8f})")
            except ValueError: pass

        else:
            # Para numéricos, usamos scatterplot
            sns.scatterplot(x=param_name, y='mean_oos_variance', data=results_df_skopt, ax=ax,
                            alpha=0.5, color='gray', label='Testes')

            # Marcar o melhor ponto com 'X'
            ax.scatter(x=[best_param_value], y=[best_score],
                       color='red', marker='X', s=200, zorder=10,
                       edgecolor='black', linewidth=1.5,
                       label=f"Melhor (Score: {best_score:.8f})")
            ax.set_title(f'Distribuição (Numérica): {param_name} vs. Score')

        ax.set_ylabel("Score (Menor = Melhor)")
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.7)

    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.suptitle('Análise de Sensibilidade 1D dos Hiperparâmetros (Distribuição de Pontos)', fontsize=16, y=1.03)
    plt.tight_layout()
    plt.show()

    # ---
    # GRÁFICO 2: ANÁLISE DE INTERAÇÃO 2D (MAPA DE CALOR/DENSIDADE)
    # ---
    print("\n--- Gráfico 2: Análise de Interação 2D (Mapa de Calor dos Melhores Scores) ---")

    # Mapear 'all' para um valor numérico para plotagem
    plot_df = results_df_skopt.copy()
    k_dim = search_space[param_names.index('max_assets_per_cluster')]
    numeric_k_values = [k for k in k_dim.categories if isinstance(k, int)]
    k_all_value = max(numeric_k_values) + 1
    k_mapping = {k: k for k in numeric_k_values}
    k_mapping['all'] = k_all_value
    k_ticks = list(numeric_k_values) + [k_all_value]
    k_tick_labels = [str(k) for k in numeric_k_values] + ['all']
    if 'max_assets_per_cluster' in plot_df.columns:
         plot_df['max_assets_per_cluster'] = plot_df['max_assets_per_cluster'].map(k_mapping)

    # Filtrar apenas os 25% melhores scores para o mapa de calor
    score_threshold = plot_df['mean_oos_variance'].quantile(0.25)
    top_25_percent_df = plot_df[plot_df['mean_oos_variance'] <= score_threshold]
    print(f"Plotando mapa de calor para os 25% melhores scores (Score <= {score_threshold:.8f})")

    pairs_to_plot = [
        ('n_clusters', 'corr_percentile'),
        ('n_clusters', 'max_assets_per_cluster'),
        ('corr_percentile', 'max_assets_per_cluster'),
        ('LOOKBACK_WINDOW', 'corr_percentile')
    ]

    n_pairs = len(pairs_to_plot)
    n_cols_pairs = 2
    n_rows_pairs = int(np.ceil(n_pairs / n_cols_pairs))

    fig_pairs, axes_pairs = plt.subplots(n_rows_pairs, n_cols_pairs, figsize=(16, n_rows_pairs * 7))
    axes_pairs = axes_pairs.flatten()

    for i, pair in enumerate(pairs_to_plot):
        ax = axes_pairs[i]
        param_x, param_y = pair

        # 1. Plotar todos os pontos (para contexto)
        sns.scatterplot(
            data=plot_df, x=param_x, y=param_y,
            color='gray', alpha=0.3, ax=ax,
            label='Testes (Todos)'
        )

        # 2. Plotar o Mapa de Calor (KDE) dos MELHORES pontos
        if len(top_25_percent_df) > 5: # KDE precisa de pontos suficientes
            sns.kdeplot(
                data=top_25_percent_df, x=param_x, y=param_y,
                ax=ax, fill=True, cmap="viridis_r", # Verde = mais denso (bom)
                levels=5,
                label='Região do "Sweet Spot" (Top 25%)'
            )

        # 3. Plotar os MELHORES pontos
        sns.scatterplot(
            data=top_25_percent_df, x=param_x, y=param_y,
            color='green', alpha=0.8, ax=ax,
            label='Testes (Top 25%)'
        )

        # Mapear o 'best_params' também para plotagem
        best_x_plot = k_mapping.get(best_params[param_x], best_params[param_x])
        best_y_plot = k_mapping.get(best_params[param_y], best_params[param_y])

        # 4. Plotar o MELHOR ponto com 'X'
        ax.scatter(
            x=[best_x_plot],
            y=[best_y_plot],
            color='red', edgecolor='black', marker='X', s=200, zorder=10,
            linewidth=1.5,
            label=f"Melhor Ponto ({best_score:.8f})"
        )

        ax.set_xlabel(param_x)
        ax.set_ylabel(param_y)
        ax.set_title(f'Interação: {param_x} vs {param_y}')
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.5)

        # Corrigir os 'ticks' do eixo se for 'max_assets_per_cluster'
        if param_x == 'max_assets_per_cluster':
            ax.set_xticks(k_ticks)
            ax.set_xticklabels(k_tick_labels)
        if param_y == 'max_assets_per_cluster':
            ax.set_yticks(k_ticks)
            ax.set_yticklabels(k_tick_labels)

    for j in range(i + 1, len(axes_pairs)):
        axes_pairs[j].axis('off')

    plt.suptitle('Análise de Interação 2D (Mapa de Calor dos Melhores Scores)', fontsize=16, y=1.03)
    plt.tight_layout()
    plt.show()

    # ---
    # GRÁFICO 3: GRÁFICO DE CONVERGÊNCIA
    # ---
    print("\n--- Gráfico 3: Convergência (SKOPT) ---")
    plot_convergence(opt_result)
    plt.title("Convergência do Score Mínimo")
    plt.show()

except Exception as e:
    print(f"\nERRO AO GERAR ANÁLISE DE SENSIBILIDADE: {e}")
    print("Certifique-se de que a Célula 16 (com 'skopt') foi executada com sucesso.")

#CELULA 17: EXECUÇÃO DO BACKTEST FINAL (WALK-FORWARD) (Versão "MIS + LW")

import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning) # Ignorar o aviso 'BQ'

print("Iniciando Célula 17: Backtest Walk-Forward...")

try:
    # 1. CONFIGURAÇÕES DO BACKTEST

    # <--- MUDANÇA CRÍTICA: Garantir que 'best_params' exista da Célula 16
    if 'best_params' not in locals():
        raise NameError("ERRO: 'best_params' não foi definido. Execute a Célula 16 primeiro.")

    PARAMS_FINAL = best_params.copy()

    # <--- MUDANÇA CRÍTICA: Garantir que 'log_returns_cv' exista da Célula 16
    if 'log_returns_cv' not in locals():
        raise NameError("ERRO: 'log_returns_cv' não foi definido. Execute a Célula 16 primeiro.")

    log_returns_backtest = log_returns_cv.copy() # Usar os mesmos dados da calibração

    LOOKBACK = PARAMS_FINAL['LOOKBACK_WINDOW']
    REBALANCE_FREQ = 'BQE' # 'BQE' é o novo 'BQ' (Business Quarter End)
    TURNOVER_COST_BPS = 0.0 # Custo zero para a otimização de pesos (como na Célula 16)
    MAX_WEIGHT = 0.10

    print(f"Iniciando backtest com parâmetros: {PARAMS_FINAL}")
    print(f"Lookback: {LOOKBACK}, Rebalance: {REBALANCE_FREQ}, Custo Otim.: {TURNOVER_COST_BPS}bps")

except NameError as e:
    print(f"ERRO DE INICIALIZAÇÃO: {e}")
    print("Execute a Célula 16 para definir 'best_params' e 'log_returns_cv' antes de rodar a 17.")
    # Parar a execução se os parâmetros essenciais não existirem
    raise

# 2. PREPARAR DATAS DE REBALANCEAMENTO
# Garantir que o índice de log_returns_backtest seja DatetimeIndex
log_returns_backtest.index = pd.to_datetime(log_returns_backtest.index)

start_date_range = log_returns_backtest.index[0]
end_date_range = log_returns_backtest.index[-1]

rebalance_dates = pd.date_range(
    start=start_date_range,
    end=end_date_range,
    freq=REBALANCE_FREQ
)

# Encontrar os dias de negociação reais mais próximos *após* as datas de rebalanceamento
rebalance_dates_idx = log_returns_backtest.index.searchsorted(rebalance_dates, side='right')
# Garantir que os índices estejam dentro dos limites
rebalance_dates_idx = [idx for idx in rebalance_dates_idx if idx < len(log_returns_backtest.index)]
rebalance_dates = log_returns_backtest.index[rebalance_dates_idx].unique()

# Filtrar datas de rebalanceamento para garantir que haja dados de lookback suficientes
first_valid_rebalance_date = log_returns_backtest.index[LOOKBACK]
rebalance_dates = rebalance_dates[rebalance_dates > first_valid_rebalance_date]

if rebalance_dates.empty:
    raise ValueError("Nenhuma data de rebalanceamento válida encontrada. Verifique o LOOKBACK_WINDOW e o período dos dados.")

print(f"Backtest de {rebalance_dates[0].date()} até {rebalance_dates[-1].date()}")
print(f"Total de {len(rebalance_dates)} rebalanceamentos.")

# 3. LOOP DE BACKTEST
all_portfolio_returns = []
previous_weights = pd.Series(dtype=float)
diagnostic_graphs = {}
diagnostic_clusters = {}
diagnostic_allocations = {}

for i, rebalance_date in enumerate(rebalance_dates):

    # Definir o período de holding (do dia seguinte ao rebalanceamento até o próximo rebalanceamento)
    hold_start_date = rebalance_date + pd.Timedelta(days=1)

    if i < len(rebalance_dates) - 1:
        hold_end_date = rebalance_dates[i+1]
    else:
        hold_end_date = log_returns_backtest.index[-1] # Ir até o fim

    hold_period_dates = log_returns_backtest.loc[hold_start_date:hold_end_date].index

    if hold_period_dates.empty:
        continue # Pular se não houver dias de negociação no período

    print(f"Rebalanceando em: {rebalance_date.date()}... Segurando até {hold_end_date.date()}", end="")

    # Definir dados de Lookback (dados ATÉ a data de rebalanceamento)
    lookback_data_full = log_returns_backtest.loc[:rebalance_date]
    if len(lookback_data_full) < LOOKBACK:
         print(" [AVISO] Pulando rebalanceamento, dados de lookback insuficientes.")
         continue

    lookback_data = lookback_data_full.iloc[-LOOKBACK:]

    try:
        # 1. Calcular Resíduos
        residuals = get_residual_returns(lookback_data, benchmark_ticker='SPY')
        valid_assets_resid = residuals.dropna(axis=1, how='any')

        if valid_assets_resid.shape[1] < PARAMS_FINAL['n_clusters']:
            raise ValueError(f"Ativos insuficientes ({valid_assets_resid.shape[1]}) para {PARAMS_FINAL['n_clusters']} clusters.")

        # 2. Fazer Clustering (com LW)
        ticker_to_cluster, dist_matrix = get_hierarchical_clusters(
            valid_assets_resid,
            n_clusters=PARAMS_FINAL['n_clusters']
        )
        diagnostic_clusters[rebalance_date] = ticker_to_cluster

        # 3. Rodar Clique no Meta-Grafo (LÓGICA CORRIGIDA)
        # <--- MUDANÇA: Chamando a função correta
        selected_clusters, G_meta = get_selected_clusters_mis(
            residuals,
            ticker_to_cluster,
            correlation_threshold_percentile=PARAMS_FINAL['corr_percentile']
        )
        diagnostic_graphs[rebalance_date] = G_meta

        if not selected_clusters:
            raise ValueError("Nenhum cluster selecionado pelo Clique.")

        # 4. Selecionar Ativos Finais
        final_assets = select_assets_from_clusters(
            selected_clusters,
            ticker_to_cluster,
            dist_matrix,
            max_assets_per_cluster=PARAMS_FINAL['max_assets_per_cluster']
        )

        if not final_assets:
            raise ValueError("Nenhum ativo selecionado dos clusters.")

        # 5. Calcular Pesos (Otimização)
        weights = get_min_variance_allocation(
            final_assets,
            lookback_data,
            max_weight_per_asset=MAX_WEIGHT,
            turnover_cost_bps=TURNOVER_COST_BPS, # Custo de turnover na otimização
            prev_weights=previous_weights
        )

        if weights.empty:
            raise ValueError("Otimização não retornou pesos.")

        print(f" -> Portfólio com {len(weights)} ativos.")
        diagnostic_allocations[rebalance_date] = weights

        # 6. Calcular Retornos OOS (Período de Holding)
        oos_returns_assets = log_returns_backtest.loc[hold_period_dates, weights.index]
        oos_returns = (oos_returns_assets.fillna(0) @ weights)
        oos_returns.name = "portfolio_return"

        all_portfolio_returns.append(oos_returns)
        previous_weights = weights # Atualizar pesos para o próximo rebalanceamento

    except Exception as e:
        print(f" -> ERRO no rebalanceamento: {e}")
        print("   -> Portfolio em caixa (retorno 0) para este período.")
        oos_returns = pd.Series(0.0, index=hold_period_dates, name="portfolio_return")
        all_portfolio_returns.append(oos_returns)
        previous_weights = pd.Series(dtype=float) # Resetar pesos

# 4. CONSOLIDAR RESULTADOS
print("\n--- BACKTEST CONCLUÍDO ---")

if all_portfolio_returns:
    portfolio_log_returns_final = pd.concat(all_portfolio_returns)
    # Remover duplicatas de índice caso um período de holding sobreponha o próximo (raro)
    portfolio_log_returns_final = portfolio_log_returns_final.loc[~portfolio_log_returns_final.index.duplicated(keep='first')]

    print(f"Retorno Logaritmico Total: {portfolio_log_returns_final.sum():.4f}")

    # Salvar resultados para as próximas células
    portfolio_log_returns_dict = {
        'Hybrid_Clique_LW': portfolio_log_returns_final
    }
    melhor_theta = 'Hybrid_Clique_LW'
    portfolio_holdings_dict = {
        melhor_theta: diagnostic_allocations
    }
    volatility_lookback_window = LOOKBACK

else:
    print("\n--- BACKTEST FALHOU: Nenhum retorno foi gerado. ---")
    portfolio_log_returns_final = pd.Series(dtype=float)

# CÉLULA 18: VISUALIZAÇÃO E DIAGNÓSTICO (AJUSTADA PARA DENDROGRAMA)

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage
import networkx as nx
import pandas as pd
import numpy as np

print("Iniciando Célula 18: Visualização de Diagnóstico...")

# --- 1. VISUALIZAR UM DENDROGRAMA (EXEMPLO) ---
try:
    if not diagnostic_graphs:
        raise ValueError("Dicionário 'diagnostic_graphs' está vazio. Rode a Célula 17.")

    last_diagnostic_date = list(diagnostic_graphs.keys())[-1]

    print(f"\nVisualizando diagnósticos para: {last_diagnostic_date.date()}")

    # --- NOVO: Plotar Dendrograma APENAS PARA OS ATIVOS DO PORTFÓLIO FINAL ---
    final_weights = diagnostic_allocations[last_diagnostic_date]
    final_portfolio_assets = final_weights.index.tolist()

    if not final_portfolio_assets:
        print("Aviso: Portfólio final vazio, não é possível plotar dendrograma detalhado.")
    else:
        # Usar apenas os retornos dos ativos no portfólio final
        lookback_data_filtered = log_returns_backtest.loc[:last_diagnostic_date, final_portfolio_assets].iloc[-LOOKBACK:]

        # Recalcular resíduos APENAS para esses ativos
        # Precisamos garantir que 'SPY' ainda esteja presente para o cálculo de resíduos,
        # mas ele será descartado da matriz de correlação.
        temp_data_for_residuals = log_returns_backtest.loc[:last_diagnostic_date, final_portfolio_assets + ['SPY']].iloc[-LOOKBACK:]
        residuals_filtered = get_residual_returns(temp_data_for_residuals, benchmark_ticker='SPY')
        valid_assets_resid_filtered = residuals_filtered.dropna(axis=1, how='any')

        if valid_assets_resid_filtered.shape[1] < 2: # Precisa de pelo menos 2 ativos para correlação
            print(f"Aviso: Menos de 2 ativos válidos no portfólio final para clustering. Ativos: {valid_assets_resid_filtered.columns.tolist()}")
        else:
            corr_matrix_filtered = valid_assets_resid_filtered.corr(method='pearson').fillna(0)
            dist_matrix_filtered = np.sqrt(0.5 * (1 - corr_matrix_filtered))

            dist_vector_condensed_filtered = dist_matrix_filtered.values[np.triu_indices_from(dist_matrix_filtered.values, k=1)]

            # Garantir que temos dados suficientes para o linkage
            if len(dist_vector_condensed_filtered) == 0:
                print("Aviso: Dados insuficientes para calcular o linkage do portfólio final.")
            else:
                Z_filtered = linkage(dist_vector_condensed_filtered, 'average')

                plt.figure(figsize=(18, 5)) # Aumentar o tamanho para melhor visualização
                plt.title(f"Dendrograma dos Ativos do Portfólio Final (Base Resíduos) - {last_diagnostic_date.date()}")
                plt.xlabel("Ativos (Ticker)")
                plt.ylabel("Distância (Average Linkage)")

                dendrogram(
                    Z_filtered,
                    labels=corr_matrix_filtered.index.tolist(),
                    leaf_rotation=90.,
                    leaf_font_size=10., # Aumentar levemente a fonte
                    # NÃO truncar se estamos mostrando apenas os ativos do portfólio
                    # Isso deve mostrar todos os nomes de tickers
                )
                plt.tight_layout() # Ajusta automaticamente para evitar sobreposição
                plt.show()

except Exception as e:
    print(f"Erro ao plotar dendrograma: {e}")


# --- 2. VISUALIZAR O META-GRAFO (MIS) ---
try:
    # A data 'last_diagnostic_date' já foi definida acima
    G_meta = diagnostic_graphs[last_diagnostic_date]
    temp_selected_clusters = list(nx.maximal_independent_set(G_meta))

    plt.figure(figsize=(12, 6))
    plt.title(f"Meta-Grafo de Clusters - {last_diagnostic_date.date()}")

    pos = nx.spring_layout(G_meta, seed=42, k=0.5) # 'k' ajusta o espaçamento

    # Nós
    node_colors = ['#1f78b4' if n in temp_selected_clusters else '#a9a9a9' for n in G_meta.nodes]
    nx.draw_networkx_nodes(G_meta, pos, node_color=node_colors, node_size=600, alpha=0.9)

    # Arestas
    nx.draw_networkx_edges(G_meta, pos, edge_color='#ff0000', width=1.5, alpha=0.7)

    # Labels
    nx.draw_networkx_labels(G_meta, pos, font_color='white', font_weight='bold', font_size=10)

    plt.text(0.0, -1.2, "Nós Azuis = Clusters Selecionados (MIS)\nNós Cinzas = Clusters Ignorados\nArestas Vermelhas = Conflito (Alta Correlação)", ha='center', fontsize=12)
    plt.axis('off')
    plt.show()

except Exception as e:
    print(f"Erro ao plotar meta-grafo: {e}")

# --- 3. HEATMAP DE CORRELAÇÃO DO PORTFÓLIO FINAL ---
try:
    # A data 'last_diagnostic_date' já foi definida acima
    final_weights = diagnostic_allocations[last_diagnostic_date]
    final_assets = final_weights.index

    # Usar retornos BRUTOS para a correlação final
    lookback_data_raw = log_returns_backtest.loc[:last_diagnostic_date, final_assets].iloc[-LOOKBACK:]
    final_corr_matrix = lookback_data_raw.corr()

    plt.figure(figsize=(10, 8))
    plt.title(f"Matriz de Correlação do Portfólio Final - {last_diagnostic_date.date()}")
    sns.heatmap(final_corr_matrix, annot=False, cmap='viridis', vmin=-1, vmax=1)
    plt.show()

except Exception as e:
    print(f"Erro ao plotar heatmap: {e}")

import scipy.stats as ss
import pandas as pd
import numpy as np

# --- FUNÇÃO HELPER (NECESSÁRIA PARA C19 e C20) ---
def calculate_annualized_metrics(returns_series, trading_days_per_year=252):
    """
    Calcula métricas anualizadas a partir de uma série de retornos logarítmicos diários.
    Retorna: (Retorno Aritmético Anualizado, Volatilidade Anualizada, Sharpe Ratio Anualizado)
    """
    if not isinstance(returns_series, pd.Series):
        try:
            returns_series = pd.Series(returns_series.squeeze(), index=returns_series.index if hasattr(returns_series, 'index') else None)
            if returns_series.ndim > 1: raise ValueError("Conversion to 1D Series failed.")
        except Exception as e:
            print(f"Error converting input to Series: {e}")
            return np.nan, np.nan, np.nan

    if returns_series.empty or returns_series.std() == 0 or np.all(np.isnan(returns_series)):
        return 0.0, 0.0, 0.0

    # 1. Calcular Retorno Log Anualizado
    mean_daily_log_return = returns_series.mean()
    annualized_log_return = mean_daily_log_return * trading_days_per_year

    # 2. Calcular Volatilidade Anualizada (do Log-Return)
    daily_volatility = returns_series.std()
    annualized_volatility = daily_volatility * np.sqrt(trading_days_per_year)

    # 3. Converter para Retorno Aritmético Anualizado (para o Sharpe)
    annualized_arithmetic_return = np.exp(annualized_log_return) - 1

    # 4. Calcular Sharpe Ratio
    sharpe_ratio = annualized_arithmetic_return / annualized_volatility if annualized_volatility > 0 else 0.0

    return annualized_arithmetic_return, annualized_volatility, sharpe_ratio

# --- Funções de Métricas Avançadas (Baseadas em MLP) ---

def get_drawdown_and_tuw(returns_series: pd.Series):
    """
    Calcula o Drawdown e o Time under Water (Tempo Sob Água)
    Baseado em MLP, Cap. 14, Snippet 14.4
    """
    df0 = returns_series.to_frame('pnl')
    cumulative_log = df0['pnl'].cumsum()
    hwm_log = cumulative_log.expanding().max()
    log_dd = hwm_log - cumulative_log
    dd_pct = 1.0 - np.exp(-log_dd) # Converte de log-drawdown para % drawdown
    max_dd_pct = dd_pct.max()

    hwm_indices = df0[log_dd == 0].index
    if len(hwm_indices) <= 1:
        max_tuw_days = len(df0) # Nunca recuperou
    else:
        tuw_periods = (hwm_indices[1:] - hwm_indices[:-1])
        max_tuw_days = tuw_periods.max().days

    return max_dd_pct, max_tuw_days

def get_hhi(returns_series: pd.Series):
    """
    Calcula o HHI (Herfindahl-Hirschman Index) dos retornos mensais.
    Mede a concentração dos lucros.
    Baseado em MLP, Cap. 14, Snippet 14.3
    """
    if returns_series.empty:
        return np.nan

    arithmetic_returns = np.exp(returns_series) - 1

    # Usar 'BME' (Business Month End)
    monthly_returns = arithmetic_returns.resample('BME').apply(lambda x: (1+x).prod() - 1)

    positive_returns = monthly_returns[monthly_returns > 0]
    if positive_returns.empty or positive_returns.sum() == 0:
        return np.nan # Nenhum mês positivo, HHI indefinido

    weights = positive_returns / positive_returns.sum()
    hhi = (weights**2).sum()
    n = len(positive_returns)
    if n <= 1:
        return 1.0 # Concentração total
    hhi_normalized = (hhi - (1.0 / n)) / (1.0 - (1.0 / n))
    return hhi_normalized

def get_psr_and_dsr(returns_series: pd.Series, n_trials: int, benchmark_sr=0.0, trading_days_per_year=252):
    """
    Calcula o Probabilistic Sharpe Ratio (PSR) e o Deflated Sharpe Ratio (DSR)
    Baseado em MLP, Cap. 14, Seção 14.7 [cite: 5481-5483]
    n_trials: Número de combinações de parâmetros testadas na Célula 16.
    """
    T = len(returns_series)
    if T < 252: # Precisa de pelo menos 1 ano de dados
        print("Aviso: Série de retornos muito curta para DSR/PSR (< 1 ano).")
        return np.nan, np.nan

    annualized_return, annualized_vol, annualized_sr = calculate_annualized_metrics(returns_series, trading_days_per_year)
    if np.isnan(annualized_sr) or annualized_vol == 0:
        print("Aviso: Sharpe ratio é NaN ou Volatilidade é 0. Pulando PSR/DSR.")
        return np.nan, np.nan

    # Momentos dos retornos diários (log)
    skewness = ss.skew(returns_series.dropna())
    kurtosis = ss.kurtosis(returns_series.dropna(), fisher=False) # Queremos Kurtosis de Pearson (Normal=3)

    sr_daily = annualized_sr / np.sqrt(trading_days_per_year)
    benchmark_sr_daily = benchmark_sr / np.sqrt(trading_days_per_year)

    # Probabilistic Sharpe Ratio (PSR)
    psr_num = (sr_daily - benchmark_sr_daily) * np.sqrt(T - 1)
    psr_den_sq = (1 - skewness * sr_daily + ((kurtosis - 1) / 4.0) * (sr_daily**2))

    if psr_den_sq <= 0: # Evita raiz de número negativo
         print("Aviso: Denominador do PSR inválido. Pulando PSR/DSR.")
         return np.nan, np.nan

    psr_den = np.sqrt(psr_den_sq)
    if psr_den == 0 or np.isnan(psr_den):
        return np.nan, np.nan

    psr = ss.norm.cdf(psr_num / psr_den)

    # --- Calcular Deflated Sharpe Ratio (DSR) ---
    # Usamos a premissa de MLP que a variância dos SRs dos N trials é 1
    # Isso é uma premissa conservadora na ausência dos SRs de cada trial.
    var_sr_trials = 1.0
    EMC = 0.5772156649 # Constante de Euler-Mascheroni

    if n_trials < 2:
        sr_star_benchmark_daily = 0.0 # Não há múltipla testagem
    else:
        sr_star_num = np.sqrt(var_sr_trials)
        # Estimativa do SR máximo esperado sob a hipótese nula (MLP 14.7) [cite: 5483]
        sr_star_den = ( (1 - EMC) * ss.norm.ppf(1 - 1/n_trials) ) + ( EMC * ss.norm.ppf(1 - 1/(n_trials * np.e)) )
        if sr_star_den == 0:
             return psr, np.nan
        sr_star_benchmark_daily = sr_star_num * sr_star_den

    # DSR é o PSR, mas com o benchmark ajustado para múltipla testagem
    dsr_num = (sr_daily - sr_star_benchmark_daily) * np.sqrt(T - 1)
    dsr = ss.norm.cdf(dsr_num / psr_den)

    return psr, dsr

# --- Executar Análise ---
if ('melhor_theta' not in locals() or
    'portfolio_log_returns_dict' not in locals() or
    'results_df_skopt' not in locals()): # <--- MUDANÇA: Nome da variável corrigido
    print("ERRO: Rode as Células 16 (Calibração) e 17 (Backtest) primeiro.")
else:
    returns_final = portfolio_log_returns_dict[melhor_theta]

    # O número de trials é o total de combinações que testamos
    n_trials_calibracao = len(results_df_skopt) # <--- MUDANÇA: Nome da variável corrigido

    print(f"\n--- Métricas Avançadas (MLP) para Estratégia = {melhor_theta} ---")

    # 1. Risco de Cauda (Drawdown)
    max_dd, max_tuw = get_drawdown_and_tuw(returns_final)
    print(f"Drawdown Máximo (DD): {max_dd:.2%}")
    print(f"Tempo Máximo Sob Água (TuW): {max_tuw} dias")

    # 2. Concentração de Lucros
    hhi = get_hhi(returns_final)
    print(f"Concentração de Lucros (HHI): {hhi:.2%}")
    print("  (0% = lucros perfeitamente distribuídos; 100% = lucro veio de um único mês)")

    # 3. Probabilidade de Sorte
    psr, dsr = get_psr_and_dsr(returns_final, n_trials=n_trials_calibracao, benchmark_sr=0.0)
    print(f"Probabilistic Sharpe Ratio (PSR): {psr:.2%}")
    print(f"  (Probabilidade que o SR > 0, dados skew/kurtosis)")
    print(f"Deflated Sharpe Ratio (DSR): {dsr:.2%}")
    print(f"  (Probabilidade que o SR > 0, ajustado para {n_trials_calibracao} testes de calibração)")

#CÉLULA 20: ANÁLISE DE CUSTOS E TURNOVER (AJUSTADA)

def get_turnover_and_cost_adjusted_returns(portfolio_holdings: dict,
                                           portfolio_log_returns: pd.Series,
                                           cost_bps_one_way=5.0):
    """
    Calcula o turnover médio e os retornos líquidos de custos.
    Esta versão lê os pesos de 'portfolio_holdings' e não recalcula nada.
    """
    turnover_list = []
    prev_weights = pd.Series(dtype=float) # Começa com portfólio vazio
    cost_per_period = {} # Armazena o custo por período de rebalanceamento

    all_rebalance_dates = sorted(portfolio_holdings.keys())
    cost_factor = cost_bps_one_way / 10000.0 # ex: 5bps = 0.0005

    for t_date in all_rebalance_dates:

        # 1. Obter pesos-alvo para a data t (já calculados e armazenados)
        # NOTA: portfolio_holdings[t_date] JÁ é a pd.Series de pesos
        current_target_weights = portfolio_holdings[t_date]

        # 2. Obter pesos do dia anterior
        current_weights = prev_weights

        # 3. Alinhar os dois portfólios (antigo e novo)
        all_tickers = current_weights.index.union(current_target_weights.index)
        w_t0 = current_weights.reindex(all_tickers, fill_value=0.0) # Pesos antigos
        w_t1 = current_target_weights.reindex(all_tickers, fill_value=0.0) # Pesos novos

        # 4. Calcular o Turnover (one-way)
        turnover = (w_t1 - w_t0).abs().sum() / 2.0
        turnover_list.append(turnover)

        # 5. Calcular Custo de Transação
        total_trade_pct = (w_t1 - w_t0).abs().sum()
        period_cost = total_trade_pct * cost_factor
        cost_per_period[t_date] = period_cost # Custo em log-return

        # 6. Atualizar pesos anteriores para o próximo loop
        prev_weights = w_t1

    # --- Cálculo de Métricas de Custo ---
    avg_annual_turnover = np.mean(turnover_list) * 12 # Assumindo rebalanceamento mensal

    # --- Ajustar Retornos ---
    cost_series_monthly = pd.Series(cost_per_period)
    cost_series_daily = pd.Series(0.0, index=portfolio_log_returns.index)

    # Mapear custos mensais para as datas de rebalanceamento
    rebalance_dates_in_returns = cost_series_daily.index.intersection(cost_series_monthly.index)

    # Garantir que não estamos tentando localizar datas que não existem
    valid_dates = rebalance_dates_in_returns[rebalance_dates_in_returns.isin(cost_series_monthly.index)]
    cost_series_daily.loc[valid_dates] = cost_series_monthly.loc[valid_dates]

    # Subtrair custos (log-retornos são aditivos)
    net_log_returns = portfolio_log_returns - cost_series_daily

    return avg_annual_turnover, net_log_returns

# --- Executar Análise de Custo ---
if 'melhor_theta' not in locals() or 'portfolio_holdings_dict' not in locals():
    print("ERRO: Rode as Células 16 e 17 (Fase 1 e 2) primeiro.")
else:
    # Custo de 5 bps (one-way)
    custo_bps = 5.0

    # Pegar os retornos brutos (antes dos custos)
    gross_returns = portfolio_log_returns_dict[melhor_theta]

    # Pegar o dicionário de pesos (holdings)
    holdings = portfolio_holdings_dict[melhor_theta]

    turnover, net_returns = get_turnover_and_cost_adjusted_returns(
        holdings,
        gross_returns,
        cost_bps_one_way=custo_bps
    )

    print(f"\n--- Análise de Custos e Turnover (Custo = {custo_bps} bps) ---")
    print(f"Turnover Anual Médio: {turnover:.2%}")

    # Calcular Métricas Brutas e Líquidas
    gross_return, gross_vol, gross_sharpe = calculate_annualized_metrics(gross_returns)
    net_return, net_vol, net_sharpe = calculate_annualized_metrics(net_returns)

    print(f"\nSharpe Ratio Bruto (Gross): {gross_sharpe:.4f}")
    print(f"Sharpe Ratio Líquido (Net): {net_sharpe:.4f}")
    print(f"Impacto do Custo no Sharpe: {gross_sharpe - net_sharpe:.4f}")

    print("\n--- Métricas de Performance (Bruto vs Líquido) ---")
    results_table = pd.DataFrame({
        'Bruto (Gross)': [f"{gross_return:.2%}", f"{gross_vol:.2%}", f"{gross_sharpe:.3f}"],
        'Líquido (Net)': [f"{net_return:.2%}", f"{net_vol:.2%}", f"{net_sharpe:.3f}"]
    }, index=['Retorno Anualizado', 'Volatilidade Anualizada', 'Índice de Sharpe'])

    print(results_table)

#CELULA 21: MÉTRICAS E COMPARATIVO DE PERFORMANCE VS BENCHMARK (SPY)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print("Iniciando Célula 21: Métricas e Gráfico vs. Benchmark (SPY)...")

try:
    # 1. Verificar se os dados e funções necessários existem
    if 'calculate_annualized_metrics' not in locals():
        raise NameError("ERRO: 'calculate_annualized_metrics' (C19) não encontrada. Rode a Célula 19.")

    if 'log_returns_df' not in locals():
        raise NameError("ERRO: 'log_returns_df' (C13) não encontrado. Rode a Célula 13.")

    # 2. Obter retornos da estratégia (LÍQUIDO ou BRUTO)
    if 'net_log_returns' in locals() and not net_log_returns.empty:
        strategy_log_returns = net_log_returns.copy()
        strategy_label = 'Estratégia Hybrid MIS (Líquido)'
        print("\nUsando 'net_log_returns' (retornos líquidos) da Célula 20.")
    elif 'portfolio_log_returns_final' in locals() and not portfolio_log_returns_final.empty:
        strategy_log_returns = portfolio_log_returns_final.copy()
        strategy_label = 'Estratégia Hybrid MIS (Bruto)'
        print("\nAVISO: Célula 20 não foi rodada. Usando 'portfolio_log_returns_final' (retornos brutos) da C17.")
    else:
        raise NameError("ERRO: 'portfolio_log_returns_final' (C17) não encontrado. Rode o backtest (C17) primeiro.")


    # 3. Obter retornos do benchmark (SPY) e alinhar
    if 'SPY' not in log_returns_df.columns:
        raise ValueError("ERRO: Coluna 'SPY' não encontrada em 'log_returns_df'.")

    benchmark_log_returns = log_returns_df['SPY'].loc[strategy_log_returns.index]

    # 4. Calcular Métricas Lado-a-Lado
    strat_ann_ret, strat_ann_vol, strat_ann_sharpe = calculate_annualized_metrics(strategy_log_returns)
    bench_ann_ret, bench_ann_vol, bench_ann_sharpe = calculate_annualized_metrics(benchmark_log_returns)

    metrics_data = {
        strategy_label: [strat_ann_ret, strat_ann_vol, strat_ann_sharpe],
        'Benchmark (SPY)': [bench_ann_ret, bench_ann_vol, bench_ann_sharpe]
    }
    metrics_df = pd.DataFrame(metrics_data, index=['Retorno Anualizado', 'Volatilidade Anualizada', 'Índice de Sharpe'])

    # --- AJUSTE NA FORMATAÇÃO ---
    # Criar um DataFrame de exibição (formatado)
    display_df = metrics_df.copy()

    # Formatar Linhas de %
    display_df.loc['Retorno Anualizado'] = display_df.loc['Retorno Anualizado'].apply(lambda x: f"{x:.2%}")
    display_df.loc['Volatilidade Anualizada'] = display_df.loc['Volatilidade Anualizada'].apply(lambda x: f"{x:.2%}")

    # Formatar Linha de Sharpe (Decimal)
    display_df.loc['Índice de Sharpe'] = display_df.loc['Índice de Sharpe'].apply(lambda x: f"{x:.3f}")

    print("\n--- Métricas de Performance (Lado-a-Lado) ---")
    # Imprimir o DataFrame formatado
    print(display_df.to_string(justify='right'))
    # ---------------------------------

    # 5. Calcular curvas de patrimônio (equity curves) normalizadas
    strategy_equity_norm = np.exp(strategy_log_returns.cumsum())
    strategy_equity_norm = strategy_equity_norm / strategy_equity_norm.iloc[0]

    benchmark_equity_norm = np.exp(benchmark_log_returns.cumsum())
    benchmark_equity_norm = benchmark_equity_norm / benchmark_equity_norm.iloc[0]

    # 6. Plotar
    plt.figure(figsize=(14, 7))

    plt.plot(strategy_equity_norm.index, strategy_equity_norm, label=strategy_label, color='blue', linewidth=2)
    plt.plot(benchmark_equity_norm.index, benchmark_equity_norm, label='Benchmark (SPY)', color='red', linestyle='--')

    plt.title(f'Performance Cumulativa: Estratégia vs. Benchmark (Início: {strategy_equity_norm.index[0].date()})')
    plt.xlabel('Data')
    plt.ylabel('Patrimônio Normalizado (Início = 1.0)')
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"\nERRO AO GERAR GRÁFICO/MÉTRICAS: {e}")