# -*- coding: utf-8 -*-
"""itauquant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oydPNWYiYTRBQaGYHTYhYO6OOy1jRJRQ

pip install yfinance newsapi-python praw fredapi openpyxl
"""

# bibliotecas
import yfinance as yf
import praw
from fredapi import Fred  # Biblioteca oficial do FRED
import pandas as pd
import datetime
import logging             # Para logs mais limpos
from google.colab import userdata
from pytrends.request import TrendReq
import time

# --- Configuração do Logging ---
# (Assumindo que 'logging' já foi importado)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Carregar Chaves de API dos Segredos do Google Colab ---
# (Assumindo que 'from google.colab import userdata' já foi importado)
NEWS_API_KEY = userdata.get('NEWS_API_KEY')
REDDIT_CLIENT_ID = userdata.get('REDDIT_CLIENT_ID')
REDDIT_CLIENT_SECRET = userdata.get('REDDIT_CLIENT_SECRET')
FRED_API_KEY = userdata.get('FRED_API_KEY')

# --- Configurações Gerais ---
TICKER = "NVDA"
REDDIT_USER_AGENT = "script_de_coleta_por_usuario_v1" # Variável necessária para o PRAW

# --- 1. Coleta de Dados de Preço e Volume (yfinance) ---
def coletar_dados_yfinance(ticker, periodo="8y"):
    """Coleta dados de preço e volume do Yahoo Finance com tratamento de erros."""
    logging.info(f"Coletando dados de preço e volume para {ticker}...")
    try:
        dados = yf.download(ticker, period=periodo)
        if dados.empty:
            logging.warning(f"Nenhum dado encontrado para {ticker} no período {periodo}.")
            return None
        return dados
    except Exception as e:
        logging.error(f"Erro ao coletar dados do yfinance para {ticker}: {e}")
        return None

# --- 2. Coleta de Dados do Google Trends ---
def coletar_dados_google_trends(termos, timeframe='today 5-y'):
    """
    Coleta dados do Google Trends para uma lista de termos de busca.

    timeframe: ex: 'today 5-y' (últimos 5 anos), '2019-01-01 2025-09-24'
    """
    pytrends = TrendReq(hl='en-US', tz=360)

    logging.info(f"Coletando dados do Google Trends para os termos: {termos}...")

    try:
        pytrends.build_payload(termos, cat=0, timeframe=timeframe, geo='US', gprop='')
        df_trends = pytrends.interest_over_time()

        if df_trends.empty:
            logging.warning("Nenhum dado retornado do Google Trends.")
            return None

        # O Google Trends retorna 'isPartial' que não queremos
        if 'isPartial' in df_trends.columns:
            df_trends.drop(columns=['isPartial'], inplace=True)

        # --- ATENÇÃO: TRATAMENTO DE DADOS SEMANAIS ---
        # Para longos períodos, a API retorna dados semanais. Precisamos converter para diário.
        # Usamos ffill() para assumir que o interesse da semana se mantém constante até a próxima medição.
        df_trends_daily = df_trends.resample('D').ffill()

        logging.info("Dados do Google Trends coletados e reamostrados para diário com sucesso.")
        return df_trends_daily

    except Exception as e:
        logging.error(f"Ocorreu um erro ao buscar dados do Google Trends: {e}")
        return None

# --- 3. Coleta de Dados do Reddit (PRAW) ---
def coletar_dados_reddit(subreddits, query, limite_posts=100, limite_comentarios=20):
    """
    Coleta dados de posts E comentários do Reddit.
    """
    logging.info(f"Coletando dados do Reddit para a query: '{query}'...")
    if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT]):
        # Mensagem de erro ATUALIZADA
        logging.error("Credenciais do Reddit não encontradas. Verifique a aba 'Segredos' do Colab.")
        return pd.DataFrame()

    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT
        )

        dados_coletados = []
        for subreddit_name in subreddits:
            try:
                subreddit = reddit.subreddit(subreddit_name)
                logging.info(f"Buscando no subreddit 'r/{subreddit_name}'...")

                # Coleta Posts
                for submission in subreddit.search(query=query, sort="relevance", limit=limite_posts):
                    dados_coletados.append({
                        'id': submission.id,
                        'tipo': 'post',
                        'subreddit': subreddit_name,
                        'titulo': submission.title,
                        'texto': submission.selftext,
                        'data': datetime.datetime.fromtimestamp(submission.created_utc),
                        'score': submission.score,
                        'num_comentarios': submission.num_comments
                    })

                    # Coleta Comentários do Post
                    submission.comments.replace_more(limit=0) # Remove "load more"
                    comentarios = submission.comments.list()
                    for comment in comentarios[:limite_comentarios]:
                        dados_coletados.append({
                            'id': comment.id,
                            'tipo': 'comentario',
                            'subreddit': subreddit_name,
                            'titulo': f"Comentário em: {submission.title[:50]}...",
                            'texto': comment.body,
                            'data': datetime.datetime.fromtimestamp(comment.created_utc),
                            'score': comment.score,
                            'num_comentarios': 0
                        })

            except Exception as e:
                logging.warning(f"Erro ao acessar subreddit '{subreddit_name}': {e}")
                continue

        df = pd.DataFrame(dados_coletados)
        # Processar datas
        if not df.empty:
            df['data'] = pd.to_datetime(df['data'])
            df.set_index('data', inplace=True)
            df.sort_index(inplace=True)

        return df

    except Exception as e:
        logging.error(f"Erro ao inicializar PRAW ou coletar dados do Reddit: {e}")
        return pd.DataFrame()

# --- 4. Coleta de Dados Econômicos (FRED API) ---
def coletar_dados_fred(series_id):
    """
    Coleta dados econômicos da API do FRED usando a biblioteca 'fredapi'.
    """
    logging.info(f"Coletando dados econômicos da série {series_id}...")
    if not FRED_API_KEY:
        # Mensagem de erro ATUALIZADA
        logging.error("Chave FRED_API_KEY não encontrada. Verifique a aba 'Segredos' do Colab.")
        return None

    try:
        fred = Fred(api_key=FRED_API_KEY)
        dados_fred_series = fred.get_series(series_id)

        # Converter a Série para DataFrame no formato desejado
        dados_fred_df = pd.DataFrame(dados_fred_series, columns=[series_id])
        dados_fred_df.index.name = 'DATE'

        return dados_fred_df
    except Exception as e:
        logging.error(f"Erro ao acessar a API do FRED para {series_id}: {e}")
        return None

# --- Silenciar Avisos do PRAW ---
# Isso diz ao Python para apenas mostrar erros da biblioteca PRAW, e não avisos.
logging.getLogger('praw').setLevel(logging.ERROR)

# --- Execução Principal do Script ---

# 1. Validação inicial das chaves
if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, FRED_API_KEY]):
    logging.error("Uma ou mais chaves de API estão faltando. Verifique a aba 'Segredos' do Colab.")
else:
    logging.info("Chaves de API carregadas com sucesso. Iniciando coleta...")

    # 2. Coleta de yfinance, google trends, reddit (como antes)
    dados_preco_volume = coletar_dados_yfinance(TICKER)

    timeframe_completo = '2019-01-01 2025-09-25'
    # Coleta 1: Interesse Geral
    dados_trends_nvidia = coletar_dados_google_trends(['Nvidia'], timeframe=timeframe_completo)
    time.sleep(1)
    # Coleta 2: Interesse do Consumidor
    dados_trends_geforce = coletar_dados_google_trends(['Geforce'], timeframe=timeframe_completo)
    time.sleep(1)
    # Coleta 3: Interesse no Ecossistema de IA
    dados_trends_cuda = coletar_dados_google_trends(['CUDA'], timeframe=timeframe_completo)

    dados_trends_final = None # Inicializa a variável

    if all(df is not None for df in [dados_trends_nvidia, dados_trends_geforce, dados_trends_cuda]):
      logging.info("Juntando os DataFrames do Google Trends...")
      # Usa o .join() para unir os três pela data (índice)
      dados_trends_final = dados_trends_nvidia.join([dados_trends_geforce, dados_trends_cuda], how='outer')
      dados_trends_final = dados_trends_final.ffill()
      dados_trends_final.dropna(inplace=True)

    dados_reddit = coletar_dados_reddit(subreddits=['investing', 'wallstreetbets'], query=TICKER)

    # --- 3. Coleta Integrada de Dados do FRED ---
    logging.info("Iniciando coleta integrada de dados do FRED...")

    # Dicionário de séries do FRED que queremos e seus nomes "amigáveis"
    series_fred = {
    # --- Setorial (O que você já tem) ---
    'NASDAQSOX': 'Indice_Semicondutores',

    # --- Índices de Mercado Amplos ---
    'NASDAQCOM': 'Indice_Nasdaq_Composite', # Índice de tecnologia, muito relevante para NVDA
    'SP500': 'Indice_SP500',                # O mercado de ações geral dos EUA

    # --- Taxas de Juros & Spreads ---
    'DGS10': 'Juros_10A_EUA',             # O que você já tem (Juros de longo prazo)
    'DFF': 'Juros_Fed_Funds',             # A taxa de juros básica do banco central (diária)
    'T10Y2Y': 'Spread_Curva_Juros',        # (Juros 10A - Juros 2A) Famoso previsor de recessão

    # --- Volatilidade e Moeda ---
    'VIXCLS': 'Indice_Medo_VIX',             # O que você já tem
    'DTWEXBGS': 'Indice_Dolar_USD',          # Força do dólar (afeta lucros de vendas no exterior)

    # --- Inflação e Commodities ---
    'T10YIE': 'Expectativa_Inflacao_10A',  # O que você já tem
    'DCOILWTICO': 'Preco_Petroleo_WTI'     # Preço do Petróleo (proxy de inflação e custos de energia)
    }

    lista_dataframes_fred = []

    # Coleta cada série e armazena numa lista
    for series_id, nome_coluna in series_fred.items():
        logging.info(f"Coletando série do FRED: {series_id}...")
        dados_novos = coletar_dados_fred(series_id)
        if dados_novos is not None:
            # Renomeia a coluna para o nome amigável
            dados_novos.rename(columns={series_id: nome_coluna}, inplace=True)
            lista_dataframes_fred.append(dados_novos)

    # --- 4. Combinação e Limpeza dos Dados FRED ---
    if lista_dataframes_fred:
        # Junta todos os DataFrames da lista usando a data (índice)
        # how='outer' garante que todas as datas de todos os arquivos sejam mantidas
        dados_fred_integrados = lista_dataframes_fred[0].join(lista_dataframes_fred[1:], how='outer')

        logging.info("Processando dados integrados do FRED (preenchendo vazios)...")

        # 1. Ordena pelo índice (data) para garantir
        dados_fred_integrados.sort_index(inplace=True)

        # 2. Preenche 'NaN's (de fins de semana/feriados) com o último valor válido
        # ffill = "forward fill"
        dados_fred_integrados = dados_fred_integrados.ffill()

        # 3. Corta o início (opcional, mas recomendado)
        # O índice NASDAQSOX começa em 2004, não precisamos de dados de juros de 1960
        # (Baseado na sua captura de tela, o índice começa em 2004-09-02)
        dados_fred_integrados = dados_fred_integrados.loc['2004-09-02':]

        # 4. Remove quaisquer NaN que sobraram no início (antes de todas as séries começarem)
        dados_fred_integrados.dropna(inplace=True)

        logging.info("Dados do FRED integrados e limpos com sucesso.")

    else:
        logging.warning("Nenhum dado do FRED foi coletado.")
        dados_fred_integrados = None

    # --- 5. Visualização dos Dados Coletados ---
    logging.info("\n--- VISUALIZAÇÃO DOS DADOS COLETADOS ---")

    if dados_preco_volume is not None:
        logging.info("\nDados de Preço e Volume (yfinance):")
        print(dados_preco_volume.head())

    if dados_trends_final is not None:
        print("\n--- DADOS FINAIS DO GOOGLE TRENDS ---")
        print(dados_trends_final.head())
        print("\nÚltimas 5 linhas:")
        print(dados_trends_final.tail())
        print(dados_trends_final.describe().round(2)) # Para verificar as novas estatísticas

    if not dados_reddit.empty:
        logging.info("\nDados do Reddit (Posts e Comentários):")
        print(dados_reddit.head())

    if dados_fred_integrados is not None and not dados_fred_integrados.empty:
        logging.info("\nDados Integrados do FRED (Primeiras 5 linhas):")
        print(dados_fred_integrados.head())
        logging.info("\nDados Integrados do FRED (Últimas 5 linhas):")
        print(dados_fred_integrados.tail())

    # --- 6. Salvamento dos Dados ---
    logging.info("\n--- SALVANDO ARQUIVOS CSV ---")

    if dados_preco_volume is not None:
        dados_preco_volume.to_csv("nvda_preco_volume.csv")
        logging.info("Salvo: nvda_preco_volume.csv")

    if dados_trends_final is not None:
        dados_trends_final.to_csv("nvda_google_trends.csv")
        logging.info("Salvo: nvda_google_trends.csv")

    if not dados_reddit.empty:
        dados_reddit.to_csv("nvda_reddit.csv")
        logging.info("Salvo: nvda_reddit.csv")

    if dados_fred_integrados is not None and not dados_fred_integrados.empty:
        dados_fred_integrados.to_csv("fred_dados_macro_integrados.csv")
        logging.info("Salvo: fred_dados_macro_integrados.csv")

    logging.info("\nColeta e salvamento concluídos!")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

try:
    # Carregar o arquivo CSV que você gerou
    df_trends = pd.read_csv('nvda_google_trends.csv', index_col='date', parse_dates=True)

    print("--- Arquivo 'nvda_google_trends.csv' carregado com sucesso! ---\n")

    # --- 1. Análise Estatística ---
    print("--- Estatísticas Descritivas ---")
    print("Isso mostra a média, desvio padrão, mínimo e máximo de cada termo.")
    print(df_trends.describe().round(2))

    # --- 2. Análise de Correlação ---
    correlation_matrix = df_trends.corr()
    print("\n--- Matriz de Correlação entre os Termos ---")
    print("Valores perto de 1.0 = muito parecidos. Valores mais baixos = sinais diferentes.")
    print(correlation_matrix.round(2))

    # --- 3. Análise Visual ---
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(16, 8))

    df_trends.plot(ax=ax, linewidth=2)

    ax.set_title('Interesse de Busca no Google Trends para Termos da NVIDIA', fontsize=18, weight='bold')
    ax.set_ylabel('Nível de Interesse (Normalizado 0-100)', fontsize=12)
    ax.set_xlabel('Data', fontsize=12)
    ax.legend(title='Termos de Busca', fontsize='large')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)

    plt.show()

except FileNotFoundError:
    print("❌ ERRO: O arquivo 'nvda_google_trends.csv' não foi encontrado.")
except Exception as e:
    print(f"❌ Ocorreu um erro durante a análise: {e}")